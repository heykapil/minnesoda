CONTENTS {{{

Linear systems and gaussian elimintation     @ 7  `LA-1`
 │ Linear systems and their solutions        @ 7  `LA-1-1`
 │ Elementary row operations                @ 12  `LA-1-2`
 │ Row echelon forms                        @ 14  `LA-1-3`
 │ Gaussian elimintation                    @ 18  `LA-1-4`
 └ Homogeneous linear systems               @ 29  `LA-1-5`

Matrices                                    @ 40  `LA-2`
 │ Introduction to matrices                 @ 40  `LA-2-1`
 │ Matrix operations                        @ 43  `LA-2-2`
 │ Inverses of square matrices              @ 52  `LA-2-3`
 │ Elementary matrices                      @ 55  `LA-2-4`
 └ Determinants                             @ 65  `LA-2-5`

Vector spaces                               @ 90  `LA-3`
 │ Euclidean n-spaces                       @ 90  `LA-3-1`
 │ Linear combinations and linear spans     @ 95  `LA-3-2`
 │ Subspaces                               @ 104  `LA-3-3`
 │ Linear independence                     @ 108  `LA-3-4`
 │ Bases                                   @ 112  `LA-3-5`
 │ Dimensions                              @ 117  `LA-3-6`
 └ Transition matrices                     @ 122  `LA-3-7`

Vector spaces associated with matrices     @ 136  `LA-4`
 │ Row spaces and column spaces            @ 136  `LA-4-1`
 │ Ranks                                   @ 145  `LA-4-2`
 └ Nullspaces and nullities                @ 147  `LA-4-3`

Orthogonality                              @ 156  `LA-5`
 │ The dot product                         @ 156  `LA-5-1`
 │ Orthogonal and orthonormal bases        @ 159  `LA-5-2`
 │ Best approximations                     @ 166  `LA-5-3`
 └ Orthogonal matrices                     @ 171  `LA-5-4`

Diagonalization                            @ 184  `LA-6`
 │ Eigenvalues and eigenvectors            @ 184  `LA-6-1`
 │ Diagonalization                         @ 191  `LA-6-2`
 │ Orthogonal diagonalization              @ 198  `LA-6-3`
 └ Quadratic forms and conic section       @ 202  `LA-6-4`

Linear transformations                     @ 216  `LA-7`
 │ Linear transformations from Rⁿ to Rᵐ    @ 216  `LA-7-1`
 │ Ranges and kernels                      @ 221  `LA-7-2`
 └ Geometric linear transformations        @ 225  `LA-7-3`

}}}

# Linear systems and gaussian elimintation {{{                     @ 7  `LA-1`
Definition 1.0.1 (Standard form) {{{

   x +  y +  z = 1
  2x -  y + 3z = 2
   x + 2y + 7z = 5
  0x - 6y + 2z = 0

}}}
Definition 1.0.2 (Matrix equation form) {{{                             (2.2.17)

𝗔𝘅 = 𝗯, where

𝗔 = [1  1  1;    𝘅 = [x;    𝗯 = [1;
     2 -1  3;         y;         2;
     1  2  7;         z]         5;
     0  6  2]                    0]  

𝗔: coefficient matrix
𝘅: variable matrix
𝗯: constant matrix

𝘂 is said to be a solution to the linear system 𝗔𝘅 = 𝗯 if 𝗔𝘂 = 𝗯.

}}}
Definition 1.0.3 (Vector equation form) {{{                             (2.2.18)

x𝗮 ₁ + y𝗮₂ + z𝗮₃ = 𝗯, where

𝗮₁ = [1;    𝗮₂ = [ 1;    𝗮₃ = [1;
      2;          -1;          3;
      1;           2;          7;
      0]           6]          2]

}}}

# Linear systems and their solutions                               @ 7  `LA-1-1`
Remark 1.1.10 {{{

Every system of linear equations has either no solution, only one solution, or
infinitely many solutions.

}}}
Discussion 1.1.11 {{{

Plane representation of 3-equation linear systems.
Best referred to at [./plane-representation.pdf]

}}}

# Elementary row operations                                       @ 12  `LA-1-2`
{{{
They can only be one of:
- Rᵢ + aRⱼ, a ∈ R, i ≠ j
- cRᵢ, c ≠ 0
- Rᵢ <-> Rⱼ, i ≠ j
}}}
Definition 1.2.6 {{{

Two augmented matrices are said to be row equivalent if one can be ontained from
the other by a series of elementary row operations.

}}}
Definition 1.2.7 {{{

If augmented matrices of two linear systems are row equivalent, then the two
systems share the same set of solutions.

}}}

# Row echelon forms                                               @ 15  `LA-1-3`
Definition 1.3.1 {{{

An augmented matrix is said to be in row-echelon form if:
- all rows consisting of only zeros are at the bottom.
- the leading coefficient (pivot) of a non-zero row is always strictly to the
  right of the leading coefficient of the row above it.

An augmented matrix is said to be in reduced row-echelon form if:
- must already by in row-echelon form.
- leading entry in each non-zero row is a 1 (leading 1)
- each column containing a leading 1 has zeros in all other entries (pivot
  column)

}}}
{{{
row echelon form, but not in reduced row-echelon form:
[1  a₀  a₁  a₂  a₃;
 0   0   2  a₄  a₅;
 0   0   0   1  a₆]

The row echelon form of a matrix is not unique,
while the reduced row-echelon form is unique.
}}}

# Gaussian elimintation                                           @ 18  `LA-1-4`
Algorithm 1.4.2 (Gaussian Elimination) {{{

reduces a matrix to row-echelon form.

}}}
Algorithm 1.4.3 (Gauss-Jordan Elimination) {{{

reduces a matrix to reduced row-echelon form.

}}}
Remark 1.4.8.1 {{{

A linear system has no solution (inconsistent) if:
the last column of a row-echelon form of the augmented matrix is a pivot column,
i.e. there is a row with a non-zero last entry but zero elsewhere.

}}}
Remark 1.4.8.2 {{{

A consistent linear system has only one solution if:
except the last column, every column if a row-echelon form of the augmented
matrix is a pivot column.

In other words, a consistent linear system has exactly one solution if:  
number of variables = number of non-zero rows in REF.

}}}
Remark 1.4.8.3 {{{

A consistent linear system has infinitely many solutions if:
apart from the last column, a row-echelon form of the augmented matrix has at
least one more non-pivot column.

In other words, a consistent linear system has infinitely many solutions if:  
number of variables > number of non-zero rows in row-echelon form.

}}}

# Homogeneous linear systems                                      @ 29  `LA-1-5`
Definition 1.5.1 {{{

A linear system is said to be homogeneous if it has only zeros as constants.

}}}
Remark 1.5.4.1 {{{

A homogeneous linear system has either only the trivial solution or infinitely
many solutions in addition to the trivial solution.

}}}
Remark 1.5.4.2 {{{

A homogeneous linear system with more unknowns than equations has infinitely
many solutions.

}}}
}}}
# Matrices {{{                                                    @ 40  `LA-2`

# Introduction to matrices                                        @ 40  `LA-2-1`
Definition 2.1.1 {{{

 1. Matrix: a rectangular array of numbers.
 2. Entries: numbers in the array.
 3. Size: given by m × n where
    m = # of rows
    n = # of columns.
 4. (i, j)-entry of a matrix is the number at the iᵗʰ row and jᵗʰ column.

}}}
Definition 2.1.3 {{{

Column matrix/vector is a matrix with only one column.

}}}
Definition 2.1.7 {{{

square matrix: a matrix where row = column.
- diagonal entries: entries where index of row and column are the same.

diagonal matrix: a matrix where all non-diagonal entries = 0.

scalar matrix: diagonal matrix with all diagonal entries the same.

identity matrix: diagonal matrix with all diagonal entries = 1. Denoted by 𝗜.

zero matrix: all entries = 0. Denoted by 𝟬.

symmetric matrix: a matrix which remains the same when transposed.

upper triangular matrix: all entries below diagonal are zero.

lower triangular matrix: all entries above diagonal are zero.

}}}

# Matrix operations                                               @ 43  `LA-2-2`
Definition 2.2.1 {{{

Two matrices are said to be equal if they have the same size and their
corresponding entries are equal.

}}}
Definition 2.2.3 {{{

Let 𝗔 = (aᵢⱼ)ₘₓₙ, 𝗕 = (bᵢⱼ)ₘₓₙ and a scalar c ∈ R.
 1. (matrix addition)    𝗔 + 𝗕 = (aᵢⱼ + bᵢⱼ)ₘₓₙ
 2. (matrix subtraction) 𝗔 + 𝗕 = (aᵢⱼ - bᵢⱼ)ₘₓₙ
 3. (scalar multiplication) c𝗔 = (caᵢⱼ)ₘₓₙ

}}}
Theorem 2.2.6 {{{

Let 𝗔, 𝗕, 𝗖 be matrices of the same size, and c, d ∈ R.
 1. (commutative) 𝗔 + 𝗕 = 𝗕 + 𝗔
 2. (associative) 𝗔 + (𝗕 + 𝗖) = (𝗔 + 𝗕) + 𝗖
 3. c(𝗔 + 𝗕) = c𝗔 + c𝗕
 4. (c + d)𝗔 = c𝗔 + d𝗔
 5. c(d𝗔) = (cd)𝗔 = d(c𝗔)
 6. 𝗔 + 𝟬 = 𝟬 + 𝗔 = 𝗔
 7. 𝗔 - 𝗔 = 𝟬
 8. 0𝗔 = 𝟬

}}}
Definition 2.2.8 (matrix multiplication) {{{

Let 𝗔 = (aᵢⱼ)ₘₓₚ, 𝗕 = (bᵢⱼ)ₚₓₙ.
The product 𝗔𝗕 is defined to be an m × n matrix whole (i, j)-entry is

𝝨_{k = 1}^{p} (aᵢₖbₖⱼ)

(dot product of transposed iᵗʰ row of left matrix and jᵗʰ row of right matrix)

}}}
Remark 2.2.10.1 {{{

We can only multiply two matrices 𝗔 and 𝗕 (in the manner 𝗔𝗕) when the number of
columns of 𝗔 is equal to the number of rows of 𝗕.

}}}
Remark 2.2.10.2 {{{

Matrix multiplication is not commutative. In general, 𝗔𝗕 and 𝗕𝗔 are two
different matrices.

}}}
Remark 2.2.10.3 {{{

𝗔𝗕 is referred to as the pre-multiplication of 𝗔 to 𝗕.
𝗕𝗔 is referred to as the post-multiplication of 𝗔 to 𝗕.

}}}
Remark 2.2.10.4 {{{

𝗔𝗕 = 𝟬 does not imply 𝗔 = 𝟬 or 𝗕 = 𝟬.

𝗔𝗕 = [0]
one possibility is 𝗔 = [1 0] and 𝗕 = [0; 1]

}}}
Remark 2.2.11 {{{

 1. (associative)  𝗔(𝗕𝗖) = (𝗔𝗕)𝗖
 2. (distributive) 𝗔(𝗕 + 𝗖) = 𝗔𝗕 + 𝗔𝗖
                   (𝗔 + 𝗕)𝗖 = 𝗔𝗖 + 𝗕𝗖

}}}
Definition 2.2.12 (powers of square matrices) {{{

Let 𝗔 be a square matrix and n be a non-negative integer.

𝗔ⁿ = 𝗜          (case: n = 0)
   = 𝗔 𝗔 … 𝗔    (case: n > 0)
     n times

}}}
Remark 2.2.14 {{{

 1. 𝗔ᵐ𝗔ⁿ = 𝗔ᵐ⁺ⁿ
 2. Since matrix multiplication is not commutative, in general,
    (𝗔𝗕)ⁿ ≠ 𝗔ⁿ 𝗕ⁿ <=> (𝗔𝗕)(𝗔𝗕) … (𝗔𝗕) ≠ (𝗔 𝗔 … 𝗔)(𝗕 𝗕 … 𝗕)

}}}
Definition 2.2.17 {{{

matrix equation form (refer to 1.0.2)

}}}
Definition 2.2.18 {{{

vector equation form (refer to 1.0.3)

}}}
Definition 2.2.19 {{{

Let 𝗔 = (aᵢⱼ)ₘₓₙ. The transpose of 𝗔 (𝗔ᵀ in writing, 𝗔' in octave) is the
n × m matrix whose (i, j)-entry is aⱼᵢ.

}}}
Theorem 2.2.22 {{{

- (𝗔ᵀ)ᵀ = 𝗔
- (𝗔 + 𝗕)ᵀ = 𝗔ᵀ + 𝗕ᵀ
- (c𝗔)ᵀ = c(𝗔ᵀ)
- (𝗔𝗕)ᵀ = 𝗕ᵀ𝗔ᵀ
-
- }}}

# Inverses of square matrices                                     @ 52  `LA-2-3`
Definition 2.3.2 {{{

Let 𝗔 be a square matrix of order n. Then 𝗔 is said to be invertible if there
exists a square matrix 𝗕 of order n such that 𝗔𝗕 = 𝗜 and 𝗕𝗔 = 𝗜. 𝗕 is called the
inverse of 𝗔. 𝗔 matrix is singular if it has no inverse.

}}}
Remark 2.3.4 {{{

 1. (cancellation law) Let 𝗔 be an invertible square matrix.
    𝗔𝗕 = 𝗔𝗖 <=> 𝗕 = 𝗖
    𝗕𝗔 = 𝗖𝗔 <=> 𝗕 = 𝗖

}}}
Theorem 2.3.5 (uniqueness of inverse) {{{

If 𝗕 and 𝗖 are inverses of a square matrix 𝗔, then 𝗕 = 𝗖

}}}
Theorem 2.3.9 {{{

Let 𝗔, 𝗕 be two invertible matrices of the same size, and c be a
non-zero scalar.

 1. c𝗔 is invertible and (c𝗔)⁻¹ = 1/c (𝗔⁻¹)
 2. 𝗔ᵀ is invertible and (𝗔ᵀ)⁻¹ = (𝗔⁻¹)ᵀ
    {{{
      𝗜 = 𝗜ᵀ = (𝗔⁻¹𝗔)ᵀ = 𝗔ᵀ(𝗔⁻¹)ᵀ
      Now since 𝗔ᵀ times (𝗔⁻¹)ᵀ gives 𝗜,
      (𝗔⁻¹)ᵀ is the inverse of 𝗔ᵀ
      => (𝗔ᵀ)⁻¹ = (𝗔⁻¹)ᵀ
    }}}
 3. 𝗔⁻¹ is invertible and (𝗔⁻¹)⁻¹ = 𝗔
    {{{
      Let (𝗔⁻¹)⁻¹ = 𝗕
      => 𝗕 is the inverse of 𝗔⁻¹,
      => 𝗕𝗔⁻¹ = 𝗜
      By definition, 𝗕 = 𝗔 satisfies that.
      Since all inverses are unique,
      𝗕 can only take the value of 𝗔.
      => (𝗔⁻¹)⁻¹ = 𝗔
    }}}
 4. 𝗔𝗕 is invertible and (𝗔𝗕)⁻¹ = 𝗕⁻¹𝗔⁻¹
    {{{
      firstly, (𝗔𝗕)⁻¹ ≠ 𝗔⁻¹𝗕⁻¹
      𝗔𝗕𝗕⁻¹𝗔⁻¹ = 𝗔(𝗕𝗕⁻¹)𝗔⁻¹
               = 𝗔𝗜𝗔⁻¹
               = 𝗜
      => (𝗔𝗕)⁻¹ = 𝗕⁻¹𝗔⁻¹
    }}}

}}}
Remark 2.3.10 {{{

By Theorem 2.3.9.4, if 𝗔₁, 𝗔₂, …, 𝗔ₖ are invertible matrices of the same
size, then (𝗔₁ 𝗔₂ … 𝗔ₖ) is invertible
(𝗔₁𝗔₂ … 𝗔ₖ)⁻¹ = 𝗔ₖ⁻¹ … 𝗔₂⁻¹ 𝗔₁⁻¹

}}}
Examples {{{
Q:
Suppose 𝗔 is an invertible matrix of order n. Then for any 𝗯 ∈ Rⁿ, 
 1. 𝗔𝘅 = 𝗯 is consistent, and
 2. the solution to 𝗔𝘅 = 𝗯 is unique.

  1.
  Let 𝘂 = 𝗔⁻¹𝗯
  Then 𝗔𝘂 = 𝗔𝗔⁻¹𝗯 = 𝗯
  so then 𝘂 is a solution. (substitutable into 𝘅 in 𝗔𝘅 = 𝗯)
  => 1. is true. (𝗔𝘅 = 𝗯 is consistent)

  2.
  Suppose 𝘃 is a solution
  Then 𝗔𝘃 = 𝗯.
  Applying 𝗔⁻¹,
  𝗔⁻¹𝗔𝘃 = 𝗔⁻¹𝗯
  𝘃 = 𝘂
  so if we can find a solution 𝘃, it will always be the same as solution 𝘂.
  => 2. is true. (solution to 𝗔𝘅 = 𝗯 is unique)

Q:
Suppose 𝗔 is an invertible matrix of order n. Then 𝗔𝘅 = 𝟬 has only the
trivial solution.

  𝗔𝘅 = 𝟬
  => 𝗔⁻¹𝗔𝘅 = 𝗔⁻¹𝟬
  => 𝘅 = 𝟬
  since the solution is shown to be unique,
  => statement is true (𝗔𝘅 = 𝟬 has only the trivial solution)

Q:
Suppose 𝗔 and 𝗕 are two square matrices of order n. 𝗔𝗕 is invertible
if and only if 𝗔 and 𝗕 are invertible. True or false?

  (<==) 𝗔𝗕⁻¹ = 𝗕⁻¹𝗔⁻¹
    if 𝗔 and 𝗕 are invertible, then
    𝗔𝗕 𝗕⁻¹𝗔⁻¹ = 𝗜
    => 𝗕⁻¹𝗔⁻¹ is the unique inverse of 𝗔𝗕
    => 𝗔𝗕 is invertible

  (==>) ∃𝗔𝗕⁻¹ => ∃𝗖 s.t. 𝗖(𝗔𝗕) = 𝗜 = 𝗔𝗕(𝗖)
    𝗜 = 𝗖(𝗔𝗕) = (𝗖𝗔)𝗕 => 𝗖𝗔 is the unique inverse of 𝗕 => 𝗕 is invertible
    𝗜 = (𝗔𝗕)𝗖 = 𝗔(𝗕𝗖) => 𝗕𝗖 is the unique inverse of 𝗔 => 𝗔 is invertible

Q:
Let 𝗔 be a square matrix with 𝗔² + 𝗔 = 𝗜. Show that 𝗔⁻¹ = 𝗔 + 𝗜.

  Let 𝗕 be the inverse of 𝗔.
  𝗔(𝗔 + 𝗜) = 𝗔² + 𝗔
           = 𝗜
  So 𝗔 + 𝗜 is the right inverse of 𝗔, which is the unique inverse of 𝗔.

Q:
Suppose that 𝗔 and 𝗕 are invertible matrices of the same size and 𝗔 + 𝗕 is
invertible. Show that
 1. 𝗔⁻¹ + 𝗕⁻¹ is invertible, and
 2. (𝗔 + 𝗕)⁻¹ = 𝗔⁻¹(𝗔⁻¹ + 𝗕⁻¹)⁻¹𝗕⁻¹

  1.
  𝗕⁻¹(𝗔 + 𝗕)𝗔⁻¹ = (𝗕⁻¹𝗔 + 𝗜)𝗔⁻¹
                = 𝗔⁻¹ + 𝗕⁻¹
  (𝗔⁻¹ + 𝗕⁻¹)⁻¹ = 𝗔(𝗔 + 𝗕)⁻¹𝗕
  => 𝗔⁻¹ + 𝗕⁻¹ is invertible

  2.
  From the last section,
     (𝗔⁻¹ + 𝗕⁻¹)⁻¹    = 𝗔(𝗔 + 𝗕)⁻¹𝗕
  𝗔⁻¹(𝗔⁻¹ + 𝗕⁻¹)⁻¹    =  (𝗔 + 𝗕)⁻¹𝗕
  𝗔⁻¹(𝗔⁻¹ + 𝗕⁻¹)⁻¹𝗕⁻¹ =  (𝗔 + 𝗕)⁻¹

}}}

# Elementary matrices                                             @ 55  `LA-2-4`
Discussion 2.4.2 {{{

All elementary row operations can be re-written as pre-multiplying the matrix
operated on with an elementary matrix.

}}}
Definition 2.4.3 {{{

A square matrix is called an elementary matrix if it can be obtained from an
identity matrix by doing a single elementary row operation.

}}}
Remark 2.4.4 {{{

All elementary matrices are invertible and their inverses are also elementary
matrices.

}}}
Theorem 2.4.7 {{{

If 𝗔 is a square matrix, then the following statements are equivalent:

 1. 𝗔 is invertible.
 2. 𝗔𝘅 = 𝟬 has only the trivial solution.
 3. rref(𝗔) = 𝗜.
 4. 𝗔 can be expressed as a product of elementary matrices.
    {{{
      1 => 2:
      If 𝗔 is invertible, then 𝗔𝘅 = 𝟬 implies
      𝘅 = 𝗜𝘅 = 𝗔⁻¹𝗔𝘅 = 𝗔⁻¹𝟬 = 𝟬
      and hence the system has only the trivial solution 𝘅 = 𝟬.
    
      2 => 3:
      Suppose 𝗔𝘅 = 𝟬 has only the trivial solution. Since the number of columns in 𝗔
      is equal to the number of rows in 𝗔, the reduced row-echelon form of the
      augmented matrix (𝗔 | 𝟬) of the system 𝗔𝘅 = 𝟬 cannot have any zero rows
      (1.4.8.2). Hence the reduced row-echelon form of (𝗔 | 𝟬) is (𝗜 | 𝟬). Hence
      rref(𝗔) = 𝗜.
    
      3 => 4:
      Since the reduced row-echelon form of 𝗔 is 𝗜, there exist elementary matrices
      𝗘₁, 𝗘₂, … 𝗘ₖ such that
      𝗘ₖ … 𝗘₂ 𝗘₁ 𝗔 = 𝗜
      and hence
      𝗔 = 𝗘₁⁻¹ 𝗘₂⁻¹ … 𝗘ₖ⁻¹ 𝗜
        = 𝗘₁⁻¹ 𝗘₂⁻¹ … 𝗘ₖ⁻¹
      where 𝗘₁⁻¹, 𝗘₂⁻¹, …, 𝗘ₖ⁻¹ are also elementary matrices.
    
      4 => 1:
      Suppose 𝗔 is a product of elementary matrices. Since all elementary matrices
      are invertible, 𝗔 is invertible (2.3.10)
    }}}

}}}
Discussion 2.4.8 {{{

Let 𝗔 be an invertible matrix of order n and let 𝗘₁, 𝗘₂, …, 𝗘ₖ be
elementary matrices such that
𝗘ₖ … 𝗘₂ 𝗘₁ 𝗔 = 𝗜.
Post multiply 𝗔⁻¹ to both sides of the equation:
𝗘ₖ … 𝗘₂ 𝗘₁ 𝗔 𝗔⁻¹ = 𝗔⁻¹
=> 𝗔⁻¹ = 𝗘ₖ … 𝗘₂ 𝗘₁
Consider the n × 2n matrix (𝗔 | 𝗜). We have
𝗘ₖ … 𝗘₂ 𝗘₁ (𝗔 | 𝗜) = 𝗘ₖ … 𝗘₂ (𝗘₁ 𝗔 | 𝗘₁ 𝗜)
                          …
                        = (𝗘ₖ … 𝗘₂ 𝗘₁ 𝗔 | 𝗘ₖ … 𝗘₂ 𝗘₁ 𝗜)
                        = (𝗜 | 𝗔⁻¹)
This provides us a method to find the inverse of 𝗔.

}}}
Remark 2.4.10 {{{

From 2.4.7, we know that a square matrix is invertible if and only if its
reduced row-echelon form is an identity matrix. Thsi can be used to check
whether a square matrix is invertible. We actually only need to reduce the
matrix to a row-echelon form. If the row-echelon form of a square matrix has no
zero row, then it is invertible (rref will definitely be 𝗜).

}}}
Theorem 2.4.12 {{{

If 𝗔𝗕 = 𝗜, then 𝗔 and 𝗕 are both invertible, and:

 1. 𝗔⁻¹ = 𝗕
 2. 𝗕⁻¹ = 𝗔
 3. 𝗕𝗔 = 𝗜

}}}
Theorem 2.4.14 {{{

Let 𝗔 and 𝗕 be two matrices of the same order. If 𝗔 is singular, then 𝗔𝗕 and 𝗕𝗔
are singular.

}}}
Example 2.4.13 {{{

Q:
Let 𝗔 be a square matrix such that
𝗔² - 3𝗔 - 6𝗜 = 𝟬
Show that 𝗔 is invertible.

  Note that
  𝗔(𝗔 - 3𝗜) = 𝗔² - 3𝗔𝗜 = 𝗔² - 3𝗔 = 6𝗜

  So 𝗔[1/6 (𝗔 - 3𝗜)] = 𝗜 and therefore 𝗔 is invertible.

}}}

# Determinants                                                    @ 65  `LA-2-5`
Discussion 2.5.2 (cofactor) {{{

Let 𝗔 = (aᵢⱼ) be an n × n matrix. Let Mᵢⱼ be an (n - 1) × (n - 1) matrix
obtained from 𝗔 by deleting the iᵗʰ row and jᵗʰ column. Then the determinant of
𝗔 is defined as

det(𝗔) = a₁₁                             if (n == 1)
       = a₁₁𝗔₁₁ + a₁₂𝗔₁₂ + … + a₁ₙ𝗔₁ₙ    if (n >= 1)

where 𝗔ᵢⱼ = (-1)ⁱ⁺ʲ det(Mᵢⱼ)
The number 𝗔ᵢⱼ is called the (i, j)-cofactor of 𝗔.
This way of defining "determinant" is known as the cofactor expansion.

}}}
Theorem 2.5.6 (cofactor expansions) {{{

For an n × n matrix 𝗔 = (aᵢⱼ), det(𝗔) can be expressed as a cofactor expansion
using any row or column of 𝗔.

}}}
Theorem 2.5.8 {{{

If 𝗔 is a triangular matrix, then the determinant of 𝗔 is equal to the product
of the diagonal entries of 𝗔.

}}}
Theorem 2.5.10 {{{

If 𝗔 is a square matrix, then det(𝗔) = det(𝗔ᵀ).

}}}
Theorem 2.5.12 {{{

 1. The determinant of a square matrix with two identical rows is zero.
 2. The determinant of a square matrix with two identical columns is zero.

}}}
Theorem 2.5.15 {{{

 1. If 𝗕 is a square matrix obtained from 𝗔 by multiplying one row of 𝗔 by a
    constant k
    => det(𝗕) = k det(𝗔)
 2. If 𝗕 is a square matrix obtained from 𝗔 by interchanging two rows of 𝗔
    => det(𝗕) = -det(𝗔)
 3. If 𝗕 is a square matrix obtained from 𝗔 by adding a multiple of one row of 𝗔
    to another row
    => det(𝗕) = det(𝗔)
 4. Let 𝗘 be an elementary matrix of the same size as 𝗔
    => det(𝗘𝗔) = det(𝗘)det(𝗔)

}}}
Remark 2.5.16 {{{

By Theorem 2.5.15, we can use elementary row operations to transform a square
matrix to a triangular matrix and then by Theorem 2.5.8, compute the determinant
accordingly.

}}}
Theorem 2.5.19 {{{

A square matrix 𝗔 is invertible if and only if det(𝗔) ≠ 0.
{{{
  Let 𝗔 be a square matrix and 𝗘₁, 𝗘₂, … 𝗘ₖ be elementary matrices such
  that 𝗕 = 𝗘ₖ … 𝗘₂ 𝗘₁ 𝗔 is the reduced row-echelon form of 𝗔.

  by Theorem 2.5.15.4, det(𝗕) = det(𝗘ₖ) … det(𝗘₂) det(𝗘₁) det(𝗔)

  (==>) If 𝗔 is invertible, then by Theorem 2.4.7, 𝗕 = 𝗜. Hence det(𝗔) ≠ 0.

  (<==, contrapositive) Suppose 𝗔 is singular. Then 𝗕 has a row consisting
  entirely of zeros. So det(𝗕) = 0. Since det(𝗘ᵢ) ≠ 0 for all i, det(𝗔) must be
  zero.
}}}

}}}
Theorem 2.5.22 {{{

Let 𝗔 and 𝗕 be two square matrices of order n and c a scalar. Then
 1. det(c𝗔) = cⁿ det(𝗔)
 2. det(𝗔𝗕) = det(𝗔) det(𝗕)
 3. if 𝗔 is invertible, det(𝗔⁻¹) = 1/det(𝗔)
    {{{
      Suppose 𝗔 is invertible. Since 𝗔𝗔⁻¹ = 𝗜,
      det(𝗔) det(𝗔⁻¹) = det(𝗔𝗔⁻¹) = det(𝗜) = 1
      clearly.
    }}}

}}}
Theorem 2.5.24 {{{

Let 𝗔 be a square matrix of order n. Then the (classical) adjoint of 𝗔 is the
matrix n × n matrix.

adj(𝗔)  =  [𝗔₁₁ 𝗔₁₂ … 𝗔₁ₙ;  =  [𝗔₁₁ 𝗔₂₁ … 𝗔ₙ₁;
            𝗔₂₁ 𝗔₂₂ … 𝗔₂ₙ;      𝗔₁₂ 𝗔₂₂ … 𝗔ₙ₂;
             ⋮      ⋱  ⋮ ;       ⋮      ⋱  ⋮ ;
            𝗔ₙ₁ 𝗔ₙ₂ … 𝗔ₙₙ]ᵀ     𝗔₁ₙ 𝗔₂ₙ … 𝗔ₙₙ]

where 𝗔ᵢⱼ is the (i, j)-cofactor of 𝗔.

}}}
Theorem 2.5.25 {{{

Let 𝗔 be a square matrix. If 𝗔 is invertible, then
𝗔⁻¹ = 1/det(𝗔) · adj(𝗔)

Note that a similar form is

𝗔 · adj(𝗔) = det(𝗔)𝗜

where 𝗔 is not required to be invertible.

}}}
{{{
  Let 𝗔 = (aᵢⱼ) be an n × n matrix and let 𝗔[adj(𝗔)] = (bᵢⱼ). Then
  bᵢⱼ = aᵢ₁ 𝗔ⱼ₁ + aᵢ₂ 𝗔ⱼ₂ + … + aᵢₙ 𝗔ⱼₙ

  By Theorem 2.5.6, bᵢi = det(𝗔).

  Note that if i ≠ j, then bᵢⱼ = det(M) where M is a matrix with two identical
  rows, and by Theorem 2.5.12, det(M) = 0 = bᵢⱼ. So then

  𝗔[adj(𝗔)] = det(𝗔) 𝗜 => 𝗔[1/det(𝗔) adj(𝗔)] = 𝗜

  By Theorem 2.4.12, 𝗔⁻¹ = 1/det(𝗔) adj(𝗔)
}}}
Theorem 2.5.27 (Cramer's Rule) {{{

Suppose 𝗔𝘅 = 𝗯 is a linear system where 𝗔 is an n × n matrix. Let 𝗔ᵢ be the
matrix obtained from 𝗔 by replacing the iᵗʰ column of 𝗔 by 𝗯. If 𝗔 is
invertible, then the system has only one solution 𝘅, where
𝘅 = 1/det(𝗔) · [det(𝗔₁);
                det(𝗔₂);
                   ⋮   ;
                det(𝗔ₙ)]

}}}
Example 2.5.28 {{{

Let 𝘅 = [x₁; and 𝗯 = [b₁;
         x₂;          b₂;
         ⋮ ;          ⋮ ;
         xₙ]          bₙ]
Since
𝗔𝘅 = 𝗯 <=> 𝘅 = 𝗔⁻¹𝗯 = 1/det(𝗔) [adj(𝗔)] 𝗯
we have
xᵢ = 1/det(𝗔) · (b₁𝗔₁ᵢ + b₂𝗔₂ᵢ + … + bₙ𝗔ₙᵢ)
   = 1/det(𝗔) · det(𝗔ᵢ)
for i = 1, 2, …, n

}}}
}}}
# Vector spaces {{{                                               @ 90  `LA-3`

# Euclidean n-spaces                                              @ 90  `LA-3-1`
Discussion 3.1.3 {{{

An n-vector or ordered n-tuple of real numbers has the form

(u₁, u₂, …, uᵢ , …, uₙ)

where u₁, u₂, …, uₙ are real numbers. The number uᵢ in the iᵗʰ position
of an n-vector is called the iᵗʰ component or the iᵗʰ coordinate of the
n-vector.

}}}
Definition 3.1.7 {{{

The set of all n-vectors of real numbers is called the Euclidean n-space or
simply n-space. We use R to denote the set of all real numbers and Rⁿ to denote
the Euclidean n-space.

"𝘂 is a vector in Rⁿ" <=> "𝘂 is an n-vector"

i.e. 𝘂 ∈ Rⁿ <=> 𝘂 = (u₁, u₂, …, uₙ) for some u₁, u₂, …, uₙ ∈ R.

}}}
Notation 3.1.9 {{{

Let S be a finite set. |S| denotes the number of elements contained in S.

}}}
Example 3.1.10 {{{

Let 𝗔 = {1, 2, 3, 4}, 𝗕 = {(1, 2, 3, 4)}, 𝗖 = {(1, 2, 3), (2, 3, 4)}.

Then |𝗔| = 4, |𝗕| = 1, |𝗖| = 2.

}}}

# Linear combinations and linear spans                            @ 95  `LA-3-2`
Definition 3.2.1 (linear combination) {{{

Let 𝘂₁, 𝘂₂, …, 𝘂ₖ be vectors in Rⁿ. For any real numbers
c₁, c₂ , …, cₖ, the vector

c₁𝘂₁ + c₂𝘂₂ + … + cₖ𝘂ₖ

is called a linear combination of 𝘂₁, 𝘂₂, …, 𝘂ₖ.

}}}
Example 3.2.2 (summary) {{{

To determine whether a vector 𝘃 is a linear combination of vectors
𝘂₁, 𝘂₂, …, 𝘂ₖ, we form the linear system

𝘃 = c₁𝘂₁ + c₂𝘂₂ + … + cₖ𝘂ₖ

and check if it has a solution (is consistent).
One way is to use Gaussian elimintation and check for pivot rows in last column.

}}}
Definition 3.2.3 (span) {{{

Let S = {𝘂₁, 𝘂₂, …, 𝘂ₖ} be a set of vectors in Rⁿ. Then the set of all
linear combinations of 𝘂₁, 𝘂₂, …, 𝘂ₖ,

{ c₁𝘂₁ + c₂𝘂₂ + … + cₖ𝘂ₖ | c₁, c₂, …, cₖ ∈ R },

is called the linear span of S (or the linear span of 𝘂₁, 𝘂₂, …, 𝘂ₖ) and
is denoted by span(S), or span{𝘂₁, 𝘂₂, …, 𝘂ₖ}

}}}
Example 3.2.4 {{{

Q:
Show that span{(1, 1, 1), (1, 2, 0), (2, 1, 3), (2, 3, 1)} ≠ R³.

  For any vector (x, y, z) ∈ R³, we solve the vector equation

  (x, y, z) = a(1, 1, 1) + b(1, 2, 0) + c(2, 1, 3) + d(2, 3, 1)

  where a, b, c, d are variables. The linear system is

    a +  b + 2c + 2d = x
    a + 2b +  c + 3d = y
    a      + 3c +  d = z

  let its augmented matrix be M.

           1  1  2  2  |  x
  ref(M) = 0  1 -1  1  |  y - x
           0  0  0  0  |  y + z - 2x

  The system is inconsistent if y + z - 2x ≠ 0.
  For example, if (x, y, z) = (1, 0, 0) where y + z - 2x = -2 ≠ 0,
  then the system doesn't have a solution,
  => (1, 0, 0) ∉ span{(1, 1, 1), (1, 2, 0), (2, 1, 3), (2, 3, 1)}.
  Hence span{(1, 1, 1), (1, 2, 0), (2, 1, 3), (2, 3, 1)} ≠ R³.

}}}
Discussion 3.2.5 (determine whether a set of vectors spans Rⁿ) {{{

Let S = {𝘂₁, 𝘂₂, …, 𝘂ₖ} ⊆ Rⁿ where 𝘂ᵢ = (aᵢ₁, aᵢ₂, …, aᵢₙ), for
i = {1, 2, …, k}. For any 𝘃 = (v₁, v₂, …, vₙ) ∈ Rⁿ, 𝘃 is contained in
span(S) if and only if the vector equation

𝘃 = c₁ 𝘂₁ + c₂ 𝘂₂ + … + cₖ 𝘂ₖ

has a solution for c₁, c₂, …, cₖ, i.e. the linear system

  a₁₁c₁ + a₂₁c₂ + … + aₖ₁cₖ = v₁
  a₁₂c₁ + a₂₂c₂ + … + aₖ₂cₖ = v₂
                      ⋮
  a₁ₙc₁ + a₂ₙc₂ + … + aₖₙcₖ = vₙ

is consistent.

Let 𝗔 be the coefficient matrix (1.0.2) of the linear system.
 1. If a row-echelon form of 𝗔 does not have any zero row, then the linear system
    is always consistent regardless of the values of v₁, v₂, …, vₙ. And
    hence span(S) = Rⁿ.
 2. If a row-echelon form of 𝗔 has at least one zero row, then the linear system
    is not always consistent and hence span(S) ≠ Rⁿ.

}}}
Theorem 3.2.7 {{{

Let S = {𝘂₁, 𝘂₂, …, 𝘂ₖ} be a set of vectors in Rⁿ. If k < n, then S
cannot span Rⁿ.

}}}
Example 3.2.8 {{{

 1. One vector cannot span R².
 2. One vector or two vectors cannot span R³.

}}}
Theorem 3.2.9 {{{

Let S = {𝘂₁, 𝘂₂, …, 𝘂ₖ} ⊆ Rⁿ.

 1. 𝟬 ∈ span(S) where 𝟬 is the zero vector.
 2. For any 𝘃₁, 𝘃₂, …, 𝘃ᵣ ∈ span(S) and c₁, c₂, …, cᵣ ∈ R,

      c₁𝘃₁ + c₂𝘃₂ + … + cᵣ𝘃ᵣ ∈ span(S)

}}}
Theorem 3.2.10 (space as subset, subspace) {{{

Let U = {𝘂₁, 𝘂₂, …, 𝘂ₖ}
and V = {𝘃₁, 𝘃₂, …, 𝘃ₘ} be subsets of Rⁿ.

Then span(U) ⊆ span(V) if and only if
each 𝘂ᵢ is a linear combination of 𝘃₁, 𝘃₂, …, 𝘃ₘ.

}}}
Example 3.2.11.2 {{{

Q:
Let 𝘂₁ = (1, 0, 0, 1), 𝘂₂ = (0, 1, -1, 2), 𝘂₃ = (2, 1, -1, 4) and
𝘃₁ = (1, 1, 1, 1), 𝘃₂ = (-1, 1, -1, 1), 𝘃₃ = (-1, 1, 1, -1). Show that
span{𝘂₁, 𝘂₂, 𝘂₃} ⊆ span{𝘃₁, 𝘃₂, 𝘃₃}
but
span{𝘂₁, 𝘂₂, 𝘂₃} ≠ span{𝘃₁, 𝘃₂, 𝘃₃}

  To show that each 𝘂ᵢ, i = 1, 2, 3 is a linear combination of 𝘃₁, 𝘃₂, 𝘃₃, we
  solve the three linear systems together.
  linearSystems = {
    a𝘃₁ + b𝘃₂ + c𝘃₃ = 𝘂₁;
    a𝘃₁ + b𝘃₂ + c𝘃₃ = 𝘂₂;
    a𝘃₁ + b𝘃₂ + c𝘃₃ = 𝘂₃;
  }
  Written as an augmented matrix, we can convert it to row-echelon form:
  1 -1 -1 | 1 |  0 |  2       1 -1 -1 |  1 |  0 |  2
  1  1  1 | 0 |  1 |  1       0  2  2 | -1 |  1 | -1
  1 -1  1 | 0 | -1 | -1  -->  0  0  2 | -1 | -1 | -3
  1  1 -1 | 1 |  2 |  4       0  0  0 |  0 |  0 |  0
  Since all three systems are consistent, all 𝘂ᵢ are linear combinations of
  𝘃₁, 𝘃₂, 𝘃₃. So
    span{u₁, 𝘂₂, 𝘂₃} ⊆ span{𝘃₁, 𝘃₂, 𝘃₃}
  On the other hand,
  1  0  2 | 1 | -1 | -1       1  0  2 | 1 | -1 | -1
  0  1  1 | 1 |  1 |  1       0  1  1 | 1 |  1 |  1
  0 -1 -1 | 1 | -1 |  1  -->  0  0  0 | 2 |  0 |  2
  1  2  4 | 1 |  1 | -1       0  0  0 | 0 |  0 |  0
  Since not all three systems are consistent, some 𝘃ᵢ are not linear combinations
  of 𝘂₁, 𝘂₂, 𝘂₃. So span{𝘃₁, 𝘃₂, 𝘃₃} is not a subset of span{𝘂₁, 𝘂₂, 𝘂₃},
  Hence
    span{𝘂₁, 𝘂₂, 𝘂₃} ≠ span{𝘃₁, 𝘃₂, 𝘃₃}.
  (specifically, 𝘃₁ and 𝘃₃ are not linear combinations of 𝘂₁, 𝘂₂, 𝘂₃.)

}}}
Theorem 3.2.12 {{{

Let 𝘂₁, 𝘂₂, …, 𝘂ₖ be vectors in Rⁿ. If 𝘂ₖ is a linear combination of
𝘂₁, 𝘂₂, …, 𝘂ₖ₋₁, then
  span{𝘂₁, 𝘂₂, …, 𝘂ₖ₋₁} = span{𝘂₁, 𝘂₂, …, 𝘂ₖ₋₁, 𝘂ₖ}

}}}
Discussion 3.2.15 (summary) {{{

 1. Take 𝘅, 𝘂 ∈ Rⁿ where 𝘂 is a non-zero vector. The set
    L = { 𝘅 + 𝘄 | 𝘄 ∈ span{𝘂} } is called a line in Rⁿ.

 2. Take 𝘅, 𝘂, 𝘃 ∈ Rⁿ where 𝘂, 𝘃 are non-zero vectors and 𝘂 is not a scalar
    multiple of 𝘃. The set P = { 𝘅 + 𝘄 | 𝘄 ∈ span{𝘂, 𝘃} } is called a plane or
    2-plane in Rⁿ.

 3. Take 𝘅, 𝘂₁, 𝘂₂, …, 𝘂ᵣ ∈ Rⁿ. The set
    Q = { 𝘅 + 𝘄 | 𝘄 ∈ span{𝘂₁, 𝘂₂, …, 𝘂ᵣ } } is called a k-plane in Rⁿ
    where k is the "dimension" of span{𝘂₁, 𝘂₂, …, 𝘂ᵣ}. (more in `LA-3-6`)

}}}
Proofs {{{
Proof 3.2.7
Follow the notation in Discussion 3.2.5. Since k < n, a row-echelon form of the
coefficient matrix must have at least one zero row. Thus span(S) ≠ Rⁿ.

Proof 3.2.9.1
Since 𝟬 = 0 𝘂₁ + 0 𝘂₂ + … + 0 𝘂ₖ, then 𝟬 ∈ span(S).

Proof 3.2.9.2
Since 𝘃₁, 𝘃₂, …, 𝘃ᵣ ∈ span(S), each 𝘃ᵢ is a linear combination of
𝘂₁, 𝘂₂, …, 𝘂ₖ, for i = {1, 2, …, r}. Thus

c₁𝘃₁ + c₂𝘃₂ + … + cᵣ𝘃ᵣ

is also a linear combination of 𝘂₁, 𝘂₂, …, 𝘂ₖ.
=> c₁𝘃₁ + c₂𝘃₂ + … + cᵣ𝘃ᵣ ∈ span(S).

Proof 3.2.10
(==>) {
Suppose span(U) ⊆ span(V). Since U ⊆ span(U),
=> U ⊆ span(V)
=> each 𝘂ᵢ in U is a linear combination of 𝘃₁, 𝘃₂, …, 𝘃ₘ.
}
(<==) {
Suppose each 𝘂ᵢ is a linear combination of 𝘃₁, 𝘃₂, …, 𝘃ₘ. It means that
𝘂₁, 𝘂₂, …, 𝘂ₖ ∈ span(V).
Let 𝘄 be any vector in span(U).
Then by Theorem 3.2.9.2, 𝘄 is also a vector in span(V).
=> span(U) ⊆ span(V)
}
}}}

# Subspaces                                                      @ 104  `LA-3-3`
Discussion 3.3.2 (subspace, span) {{{

Let V be a subset of Rⁿ. Then V is called a subspace of Rⁿ if V = span(S)
where S = {𝘂₁, 𝘂₂, …, 𝘂ₖ} for some vectors 𝘂₁, 𝘂₂, …, 𝘂ₖ ∈ Rⁿ. More
precisely, V is called the subspace spanned by S (or the subspace spanned by
𝘂₁, 𝘂₂, …, 𝘂ₖ). We also say that S spans (or 𝘂₁, 𝘂₂, …, 𝘂ₖ span) the
subspace V.

}}}
Remark 3.3.3 {{{

 1. Let 𝟬 be the zero vector in Rⁿ. Then the set {𝟬} = span{𝟬} is a subspace of
    Rⁿ and is known as the zero space.

 2. Let 𝗲₁ = [1; 0; …; 0]; 𝗲₂ = [0; 1; …; 0]; …, 𝗲ₙ = [0; 0; …; 1] be vectors
    of Rⁿ. Any vector 𝘂 = (u₁, u₂, …, uₖ) ∈ Rⁿ can be written as
      𝘂 = u₁𝗲₁ + u₂𝗲₂ + … uₖ𝗲ₖ
    Thus Rⁿ = span{𝗲₁, 𝗲₂, …, 𝗲ₙ} is a subspace of Rⁿ.

}}}
Remark 3.3.5 {{{

 1. The following are all the subspaces of R²:
    a. {𝟬} (given by span{𝟬}),
    b. lines through the origin (given by span{𝘂} for non-zero 𝘂 ∈ R²),
    c. R² (given by span{[1; 0], [0; 1]}).

 2. The following are all the subspaces of R³:
    a. {𝟬} (given by span{𝟬}),
    b. lines through the origin (given by span{𝘂} for non-zero 𝘂 ∈ R³),
    c. planes containing the origin (given by span{𝘂, 𝘃} for non-zero 𝘂, 𝘃 ∈ R³
       which are not parallel to each other),
    d. R³ (given by span{[1; 0; 0], [0; 1; 0], [0; 0; 1]}).

}}}
Theorem 3.3.6 {{{

The solution set of a homogeneous linear system in n variables is a subspace of
Rⁿ. (This subspace is called the solution space of the system)

}}}
Remark 3.3.8 (subspace condition) {{{

Let V be a non-empty subset of Rⁿ. Then V is a subspace of Rⁿ if and only if:

∀𝘂, 𝘃 ∈ V and ∀c, d ∈ R, c𝘂 + d𝘃 ∈ V

}}}
Proofs {{{
Proof 3.3.6
If the homogeneous system has only the trivial solution, then the solution set
is {𝟬} which is the zero space.

Suppose the homogeneous system has infinitely many solutions. Let
x₁, x₂, …, xₙ be the variables of the system. By solving the system, say,
using Gauss-Jordan elimintation, a general solution can be expressed in the form

x₁ = r₁₁t₁ + r₁₂t₂ + … + r₁ₖtₖ
x₂ = r₂₁t₁ + r₂₂t₂ + … + r₂ₖtₖ
                     ⋮
xₙ = rₙ₁t₁ + rₙ₂t₂ + … + rₙₖtₖ

for some arbitrary parameters t₁, t₂, …, tₖ, where r₁₁, r₁₂, …, rₙₖ ∈ R.
We can rewrite this general solution as

[x₁; = t₁ [r₁₁; + t₂ [r₁₂; + … + tₖ [r₁ₖ;
 x₂;       r₂₁;       r₂₂;           r₂ₖ;                         
 ⋮ ;       ⋮ ;        ⋮ ;            ⋮ ;
 xₙ]       rₙ₁]       rₙ₂]           rₙₖ]

The solution set is
  span{
    [r₁₁; r₂₁; …; rₙ₁],
    [r₁₂; r₂₂; …; rₙ₂],
               ⋮      ,
    [r₁ₖ; r₂ₖ; …; rₙₖ]
  }
}}}

# Linear independence                                            @ 108  `LA-3-4`
Definition 3.4.2 {{{

Let S = {𝘂₁, 𝘂₂, …, 𝘂ₖ} be a set of vectors in Rⁿ. Consider the equation

c₁𝘂₁ + c₂𝘂₂ + … + cₖ𝘂ₖ = 𝟬

where c₁, c₂, …, cₖ are variables. Note that

c₁ = 0, c₂ = 0, …, cₖ = 0

satisfies the equation, and hence is a solution. This is called the trivial
solution.

 1. S is called a linearly independent set and 𝘂₁, 𝘂₂, …, 𝘂ₖ are said to be
    linearly independent if the equation has only the trivial solution.
 2. Otherwise:
    - S is called a linearly dependent set
    - 𝘂₁, 𝘂₂, …, 𝘂ₖ are said to be linearly dependent.
    -
}}}
Example 3.4.3 (determining linear independence) {{{

Q:
Determine whether the vectors (1, -2, 3), (5, 6, -1), (3, 2, 1) are linearly
independent.

  The equation
    c₁(1, -2, 3) + c₂(5, 6, -1) + c₃(3, 2, 1) = (0, 0, 0)
  gives us the linear system
    c₁ + 5c₂ + 3c₃ = 0
  -2c₁ + 6c₂ + 2c₃ = 0
   3c₁ -  c₂ +  c₃ = 0
  By Gaussian Elimination, we find that there are infinitely many solutions, i.e.
  there exist non-trivial solutions. So the vectors are linearly independent.

}}}
Theorem 3.4.4 {{{

Let S = {𝘂₁, 𝘂₂, …, 𝘂ₖ} be a set of vectors in Rⁿ where k >= 2. Then

 1. S is linearly dependent if and only if at least one vector 𝘂ᵢ in S can be
    written as a linear combination of other vectors in S.
 2. S is linearly independent if and only if no vector in S can be written as a
    linear combination of other vectors in S.

}}}
Theorem 3.4.7 {{{

Let S = {𝘂₁, 𝘂₂, …, 𝘂ₖ} be a set of vectors in Rⁿ. If k > n, then S is
linearly dependent.

}}}
Theorem 3.4.10 {{{

Let 𝘂₁, 𝘂₂, …, 𝘂ₖ be linearly independent vectors in Rⁿ. If 𝘂ₖ₊₁ is a
vector in Rⁿ and it is not a linear combination of 𝘂₁, 𝘂₂, …, 𝘂ₖ, then
𝘂₁, 𝘂₂, …, 𝘂ₖ, 𝘂ₖ₊₁ are linearly independent.

}}}
Proofs {{{
Proof 3.4.7 {
Form a linear system to find what linear combination of the vectors will equate
to the zero vector.
This linear system with k unknowns and n equations with k > n will have
non-trivial solutions.
}
}}}

# Bases                                                          @ 112  `LA-3-5`
Discussion 3.5.1 (vector space | subspace) {{{

From now on, we shall adopt the following conventions in using the terms "vector
space" and "subspace".

 1. A set V is called a vector space if either V = Rⁿ or V is a subspace of Rⁿ
    for some positive integer n.
 2. Let W be a vector space. A set V is called a subspace of W if V is a vector
    space contained in W.

}}}
Discussion 3.5.3 {{{

Given a vector space V, we want to find a set S, as small as possible, so that
every vector in V is a linear combination of the elements in S. Such a set can
then be used to build a "coordinate system" for V.

}}}
Definition 3.5.4 (basis) {{{

Let S = {𝘂₁, 𝘂₂, …, 𝘂ₖ} be a subset of a vector space V. Then S is called
a basis for V if

 1. S is linearly independent and 
 2. S spans V.

}}}
Remark 3.5.6 {{{

 1. A basis for a vector space V contains the smallest possible number of
    vectors that can span V.
 2. For convenience, we say that the empty set, ∅, is the basis for the zero
    space.
 3. Except the zero space, any vector space has infinitely many different bases.

}}}
Theorem 3.5.7 (uniqueness of basis representation) {{{

If S = {𝘂₁, 𝘂₂, …, 𝘂ₖ} is a basis for a vector space V,
then every vector 𝘃 ∈ V can be expressed in the form

𝘃 = c₁𝘂₁ + c₂𝘂₂ + … + cₖ𝘂ₖ

in exactly one way,
where c₁, c₂, …, cₖ ∈ R.
{{{
  Since S spans V, every vector v can be expressed as a linear combination of the
  elements of S. Suppose that a vector 𝘃 can be expressed in two ways
    𝘃 = c₁ 𝘂₁ + c₂ 𝘂₂ + … + cₖ 𝘂ₖ
    and
    𝘃 = d₁ 𝘂₁ + d₂ 𝘂₂ + … + dₖ 𝘂ₖ
  Subtracting the second equation from the first, we obtain
    (c₁ - d₁)𝘂₁ + (c₂ - d₂)𝘂₂ + … + (cₖ - dₖ)𝘂ₖ = 𝟬
  Since 𝘂₁, 𝘂₂, …, 𝘂ₖ are linearly independent, the only possible solution
  is
    c₁ - d₁ = 0, c₂ - d₂ = 0, …, cₖ - dₖ = 0
  i.e. c₁ = d₁, c₂ = d₂, …, cₖ = dₖ. So the expression is unique.
}}}

}}}
Theorem 3.5.8 {{{

Let S = {𝘂₁, 𝘂₂, …, 𝘂ₖ} be a basis for a vector space V, and 𝘃 ∈ V.
By Theorem 3.5.7,
𝘃 is expressed uniquely as a linear combination 

𝘃 = c₁𝘂₁ + c₂𝘂₂ + … + cₖ𝘂ₖ.

The coefficients c₁, c₂, …, cₖ are called
the coordinates of 𝘃 relative to the basis S.

The vector (𝘃)_S = (c₁, c₂, …, cₖ) ∈ Rᵏ is called
the coordinate vector of 𝘃 relative to the basis S.

(Here we assume the vectors of S are in a fixed order)

}}}
Example 3.5.9.3 (standard basis) {{{

Let E = {𝗲₁, 𝗲₂, …, 𝗲ₙ} where

𝗲₁ = [1, 0, …, 0],
𝗲₂ = [0, 1, …, 0],
   ⋮             ,
𝗲ₙ = [0, 0, …, 1]

are vectors of Rⁿ.

By Remark 3.3.3.2, E spans Rⁿ.

Also, it is easy to show that E is linearly independent.
Thus E is a basis for Rⁿ which is called the standard basis for Rⁿ.
For any 𝘂 = (u₁, u₂, …, uₖ) ∈ Rⁿ,

(𝘂)_E = (u₁, u₂, …, uₖ) = 𝘂

}}}
Remark 3.5.10 {{{

Let S be a basis of a vector space V.

 1. For any 𝘂, 𝘃 ∈ V,

    𝘂 = 𝘃 <=> (𝘂)_S = (𝘃)_S

 2. For any 𝘃₁, 𝘃₂, …, 𝘃ᵣ ∈ V and c₁, c₂, …, cᵣ ∈ R,

    (c₁𝘃₁ + c₂𝘃₂ + … + cᵣ𝘃ᵣ)_S = c₁(𝘃₁)_S + c₂(𝘃₂)_S + … + cᵣ(𝘃ᵣ)_S.

}}}
Theorem 3.5.11 {{{

Let S be a basis for a vector space V where |S| = k.
Let 𝘃₁, 𝘃₂, …, 𝘃ᵣ be vectors in V.
Then
 1. 𝘃₁, 𝘃₂, …, 𝘃ᵣ are linearly (in)dependent vectors in V
    <=> (𝘃₁)_S, (𝘃₂)_S, …, (𝘃ᵣ)_S are linearly (in)dependent vectors in Rᵏ
    {{{
      By Remark 3.5.10,

      c₁𝘃₁ + c₂𝘃₂ + … + cᵣ𝘃ᵣ = 𝟬
      => (c₁𝘃₁ + c₂𝘃₂ + … + cᵣ𝘃ᵣ)_S = (𝟬)_S
      => (c₁𝘃₁)_S + (c₂𝘃₂)_S + … + (cᵣ𝘃ᵣ)_S = (𝟬)_S

      where (𝟬)_S is the zero vector in Rᵏ.

      The equation c₁𝘃₁ + c₂𝘃₂ + … + cᵣ𝘃ᵣ = 𝟬 has a non-trivial solution
      if and only if the equation

      (c₁𝘃₁)_S + (c₂𝘃₂)_S + … + (cᵣ𝘃ᵣ)_S = (𝟬)_S 

      has a non-trivial solution.
    }}}
 2. span{𝘃₁, 𝘃₂, …, 𝘃ᵣ} = V
    <=> span{(𝘃₁)_S, (𝘃₂)_S, …, (𝘃ᵣ)_S} = Rᵏ
    {{{
      Let S = {𝘂₁, 𝘂₂, …, 𝘂ₖ}
        (==>) {
          Take any vector (a₁, a₂, …, aₖ) ∈ Rᵏ.
          Let 𝘄 = a₁ 𝘂₁ + a₂ 𝘂₂ + … + aₖ 𝘂ₖ ∈ V.
          Suppose span{𝘃₁, 𝘃₂, …, 𝘃ᵣ} = V.
          Then there exists real numbers c₁, c₂, …, cᵣ such that
            𝘄 = c₁𝘃₁ + c₂𝘃₂ + … + cᵣ𝘃ᵣ
          By Remark 3.5.10,
            c₁(𝘃₁)_S + c₂(𝘃₂)_S + … + cᵣ(𝘃ᵣ)_S
            = (c₁ 𝘃₁ + c₂ 𝘃₂ + … + cᵣ 𝘃ᵣ)_S
            = (𝘄)_S
            = (a₁, a₂, …, aₖ)
          This shows that every vector in Rᵏ is a linear combination of
          (𝘃₁)_S, (𝘃₂)_S, …, (𝘃ᵣ)_S.
          => span{(𝘃₁)_S, (𝘃₂)_S, …, (𝘃ᵣ)_S} = Rᵏ
        }
        (<==) {
          Suppose span{(𝘃₁)_S, (𝘃₂)_S, …, (𝘃ᵣ)_S} = Rᵏ.
          Take any vector 𝘄 ∈ V.
          Since (𝘄)_S ∈ Rᵏ,
          There exists real numbers c₁, c₂, …, cᵣ such that
            (𝘄)_S
            = c₁(𝘃₁)_S + c₂(𝘃₂)_S + … + cᵣ(𝘃ᵣ)_S
            = (c₁𝘃₁ + c₂𝘃₂ + … + cᵣ𝘃ᵣ)_S
          Hence
            𝘄 = c₁𝘃₁ + c₂𝘃₂ + … + cᵣ𝘃ᵣ
          This shows that every vector in V is a linear combination of
          𝘃₁, 𝘃₂, …, 𝘃ᵣ.
          => span{𝘃₁, 𝘃₂, …, 𝘃ᵣ} = V
        }
    }}}

}}}

# Dimensions                                                     @ 117  `LA-3-6`
Theorem 3.6.1 (vector space) {{{

Let V be a vector space which has a basis with k vectors. Then

 1. any subset of V with more than k vectors is always linearly dependent.
 2. any subset of V with less than k vectors cannot span V.

}}}
Remark 3.6.2 {{{

By Theorem 3.6.1, all bases for a vector space have the same number of vectors.
This number gives us a way to measure the "size" of a vector space.

}}}
Definition 3.6.3 (dimension) {{{

The dimension of a vector space V = number of vectors in a basis for V.
  = dim(V)

In addition, we define the dimension of the zero space to be zero.

}}}
Theorem 3.6.7 (basis check) {{{

Let V be a vector space, dim(V) = k, S ⊆ V.
The following are equivalent:

 1. S is a basis for V.
 2. S is linearly independent and |S| = k.
 3. S spans V (V ⊆ span(S)) and |S| = k.

best table of all time {{{
Equivalent ways to check if S is a basis of V {
 1. { (definition)
   V = span(S),
   S is linearly independent
 }
 2. {
   |S| = dim(V), 
   S ⊆ V
   S is linearly independent
 }
 3. {
   |S| = dim(V),
   V ⊆ span(S)
   => (inherently true) S ⊆ V
 }
}
}}}

}}}
Theorem 3.6.9 {{{

Let U be a subspace of V.
  Then dim(U) <= dim(V).
And if U ≠ V,
  then dim(U) < dim(V).
  (Contrapositive: if dim(U) >= dim(V), then U = V)

}}}
Theorem 3.6.11 (invertible matrices) {{{

This theorem continues from 2.4.7 and forms part of the main theorem 6.1.8. Let
𝗔 be an n × n matrix. The following statements are equivalent.

 1. 𝗔 is invertible.
 2. The linear system 𝗔𝘅 = 0 has only the trivial solution.
 3. The reduced row-echelon form of 𝗔 is an identity matrix.
 4. 𝗔 can be expressed as a product of elementary matrices.
 5. det(𝗔) ≠ 0.
 6. The rows of 𝗔 form a basis for Rⁿ.
 7. The columns of 𝗔 form a basis for Rⁿ.

By Theorem 2.4.7, statements 1 to 4 are equivalent.
By Theorem 2.5.19, we have 1 <=> 5

The rows of 𝗔 are columns of 𝗔ᵀ. By 2.3.9, 𝗔 is invertible <=> 𝗔ᵀ is invertible,
hence we only need to show either 1 <=> 6 or 1 <=> 7.

Let 𝗔 = (𝗮₁, 𝗮₂, …, 𝗮ₙ) where 𝗮ᵢ is the iᵗʰ column of 𝗔.

{𝗮₁, 𝗮₂, …, 𝗮ₙ} is a basis for Rⁿ
<=> span{𝗮₁, 𝗮₂, …, 𝗮ₙ} = Rⁿ           (3.6.7)
<=> a row-echelon form of 𝗔 has no zero row   (3.2.5.1)
<=> 𝗔 is invertible                           (2.4.10)

}}}

# Transition matrices                                            @ 122  `LA-3-7`
Notation 3.7.1 (relative coordinate vector) {{{

Let S = {𝘂₁, 𝘂₂, …, 𝘂ₖ} be a basis for a vector space V and
let 𝘃 be a vector in V.
Recall that if 𝘃 = c₁ 𝘂₁ + c₂ 𝘂₂ + … + cₖ 𝘂ₖ, then

(𝘃)_S = (c₁, c₂, …, cₖ)

is called the coordinate vector of 𝘃 relative to S. Sometimes, it is more
convenient to write the coordinate vector in the form of a column vector.

Thus we define

[𝘃]_S = [c₁;
         c₂;
          ⋮ ;
         cₖ]

and also call it the coordinate vector of 𝘃 relative to S.

}}}
Definition 3.7.3 (transition matrix) {{{

Let S = {𝘂₁, 𝘂₂, …, 𝘂ₖ}
and T be two bases for a vector space V.
The square matrix 𝗣 = {[𝘂₁]_T [𝘂₂]_T … [𝘂ₖ]_T} is called the transition
matrix from S to T.

So if 𝘄 ∈ V, then

[𝘄]_T = 𝗣 [𝘄]_S

𝗣 has to be a square matrix.

Side Note {
  V ⊂ Rⁿ, V = span(S) = span(T)
  S = {u₁, u₂, ..., uₘ} and T = {v₁, v₂, ..., vₘ}
  C = (u₁ u₂ ... uₘ)    and D = (v₁ v₂ ... vₘ)
   
  CᵀD = ⎡ u₁·v₁ u₁·v₂ … u₁·vₘ ⎤
        ⎢ u₂·v₁ u₂·v₂ … u₂·vₘ ⎢
        ⎢   ⋮     ⋮   ⋱   ⋮   ⎢
        ⎣ uₘ·v₁ uₘ·v₂ … uₘ·vₘ ⎦ is the transition matrix from T to S
}
}}}
Example 3.7.4 (finding transition matrix) {{{

S = {𝘂₁, 𝘂₂, 𝘂₃},
  𝘂₁ = (1, 0, -1),
  𝘂₂ = (0, -1, 0),
  𝘂₃ = (1, 0, 2).
T = {𝘃₁, 𝘃₂, 𝘃₃},
  𝘃₁ = (1, 1, 1),
  𝘃₂ = (1, 1, 0),
  𝘃₃ = (-1, 0, 0).

(a) find the transition matrix from S to T.
First, we need to find a₁₁, a₂₁, …, a₃₃ such that
a₁₁ 𝘃₁ + a₂₁ 𝘃₂ + a₃₁ 𝘃₃ = 𝘂₁
a₁₂ 𝘃₁ + a₂₂ 𝘃₂ + a₃₂ 𝘃₃ = 𝘂₂
a₁₃ 𝘃₁ + a₂₃ 𝘃₂ + a₃₃ 𝘃₃ = 𝘂₃

⎡ 1  1 -1 |  1 |  0 |  1 ⎤  rref  ⎡ 1  0  0 | -1 |  0 |  2 ⎤
⎢ 1  1  0 |  0 | -1 |  0 ⎢   -->  ⎢ 0  1  0 |  1 | -1 | -2 ⎢
⎣ 1  0  0 | -1 |  0 |  2 ⎦        ⎣ 0  0  1 | -1 | -1 | -1 ⎦
                                              `↑ 𝘂₁ in terms of elements in T`
we have
-𝘃₁ +  𝘃₂ - 𝘃₃ = 𝘂₁ `← corresponds to that`
    -  𝘃₂ - 𝘃₃ = 𝘂₂
2𝘃₁ - 2𝘃₂ - 𝘃₃ = 𝘂₃

        `↓ and finally to this`
       ⎡-1  0  2⎤
So 𝗣 = ⎢ 1 -1 -2⎢ is a transition matrix from S to T.
       ⎣-1 -1 -1⎦

`i.e. (-1, 1, -1) = (𝘂₁)_T, in line with definition 3.7.3`

}}}
}}}
# Vector spaces associated with matrices {{{                     @ 136  `LA-4`

# Row spaces and column spaces                                   @ 136  `LA-4-1`
Definition 4.1.2 (row space, column space) {{{

Let 𝗔 be an m × n matrix

The row space of 𝗔 = span{m row vectors of 𝗔} ⊆ R^m

The column space of 𝗔 = span{n column vectors of 𝗔} ⊆ Rⁿ

}}}
Remark 4.1.3 {{{

row space of 𝗔 = column space of 𝗔ᵀ
column space of 𝗔 = row space of 𝗔ᵀ

}}}
Theorem 4.1.7 (row equivalent matrices) {{{

Let 𝗔 and 𝗕 be row equivalent matrices. Then

row space of 𝗔 = row space of 𝗕

i.e. elementary row operations preserve the row space of a matrix.
{{{
  To prove this,
  let 𝗔 be an arbitrary matrix with rows {r₁, r₂, …, rₘ}
  and let 𝗕 be a matrix with one row operated on by an elementary row operation
  (rᵢ -> krᵢ, for example, for some i ∈ [1, m]).
  Show that the row space of 𝗔 and 𝗕 are subsets of each other.
}}}

}}}
Remark 4.1.9 {{{

Let 𝗔 be a matrix and R a row-echelon form of 𝗔. Then the set of non-zero rows
in R is a basis for the row space of 𝗔.

}}}
Theorem 4.1.11 (linear relations) {{{

Let 𝗔 and 𝗕 be row equivalent matrices. Then

 1. A given set of columns of 𝗔 is linearly independent if and only if the set of
    corresponding columns of 𝗕 is linearly independent.

 2. A given set of columns for 𝗔 forms a basis for the column space of 𝗔 if and
    only if the set of corresponding columns of 𝗕 forms a basis for the column
    space of 𝗕.

}}}
Remark 4.1.13 {{{

Extending theorem 4.1.11,

Let 𝗔 be a matrix and let R be a row-echelon form of 𝗔.

A basis for the column space of 𝗔 can be obtained by taking the columns of 𝗔
that correspond to the pivot columns in R.

}}}
Theorem 4.1.16 {{{

Let 𝗔 be an m × n matrix. Then

column space of 𝗔 = { 𝗔𝘂 | 𝘂 ∈ Rⁿ }

Hence ∃𝘅 where the system of linear equations 𝗔𝘅 = 𝗯 is consistent if and only
if 𝗯 lies in the column space of 𝗔.
{{{
  Write 𝗔 = (c₁ c₂ … cₙ) where cⱼ is the jᵗʰ column of 𝗔.
  For any 𝘂 = (u₁, u₂, …, uₙ)ᵀ ∈ Rⁿ,

  𝗔𝘂 = (c₁ c₂ … cₙ)(u₁, u₂, …, uₙ)ᵀ
     = u₁ c₁ + u₂ c₂ + … + uₙ cₙ
     ∈ span{c₁, c₂, …, cₙ}
     = column space of 𝗔.
  => { 𝗔𝘂 | 𝘂 ∈ Rⁿ } ⊆ column space of 𝗔.

  Conversely, suppose 𝗯 is in the column space of 𝗔.
  i.e. 𝗯 ∈ span{c₁, c₂, …, cₙ}
  then there exists u₁, u₂, …, uₙ ∈ R such that

  𝗯 = u₁ c₁ + u₂ c₂ + … + uₙ cₙ = 𝗔𝘂

  where 𝘂 = (u₁, u₂, …, uₙ)ᵀ.

  => column space of 𝗔 ⊆ { 𝗔𝘂 | 𝘂 ∈ Rⁿ }.

  so we have shown that
  column space of 𝗔 = { 𝗔𝘂 | 𝘂 ∈ Rⁿ }.

  Finally, a system of linear equations 𝗔𝘅 = 𝗯 is consistent if and only if
  there exists 𝘂 ∈ Rⁿ s.t. 𝗔𝘂 = 𝗯
  <=> 𝗯 ∈ { 𝗔𝘂 | 𝘂 ∈ Rⁿ } = column space of 𝗔.
}}}

}}}

# Ranks                                                          @ 145  `LA-4-2`
Theorem 4.2.1 {{{

The row space and column space of a matrix have the same dimension.
{{{
Let 𝗔 be a matrix and R a row-echelon form of 𝗔.
Since the row space of 𝗔 conincides with that of R (4.1.9),
we see that dim(row space of 𝗔) = number of non-zero rows in R
                                = number of pivot columns in R

On the other hand, the columns of 𝗔 that correspond to the pivot columns in R
form a basis for the column space of 𝗔 (4.1.13).
It follows that the dimension of the column space of 𝗔 is also equal to the
number of pivot columns in R.
This completes the proof.
}}}

}}}
Definition 4.2.3 (rank) {{{

The rank of a matrix is the dimension of its row space (or column space)

Denoted by rank(𝗔)

Note that rank(𝗔) also equals number of non-zero rows and number of pivot
columns in a row-echelon form of 𝗔.

}}}
Remark 4.2.5 {{{

 1. For an m × n matrix 𝗔, rank(𝗔) <= min{m, n}.
    if rank(𝗔) = min{m, n}, 𝗔 is said to have full rank.
 2. A square matrix 𝗔 is of full rank <=> det(𝗔) ≠ 0.
 3. rank(𝗔) = rank(𝗔ᵀ) for any matrix 𝗔,
    because the row space of 𝗔 is the column space of 𝗔ᵀ
 4. rank(0) = 0, rank(𝗜ₙ) = n

}}}
Remark 4.2.6 (linear systems and rank) {{{

A linear system 𝗔𝘅 = 𝗯 is consistent if and only if 𝗔 and the augmented matrix
(𝗔 | 𝗯) have the same rank.

𝗔𝘅 = 𝗯 is consistent <=> rank(𝗔) = rank(𝗔 | 𝗯)

}}}
Theorem 4.2.8 {{{

Let 𝗔 and 𝗕 be m × n and n × p matrices respectively. Then

rank(𝗔𝗕) <= min{rank(𝗔), rank(𝗕)}
{{{
Let 𝗔 = (𝗮₁ 𝗮₂ … 𝗮ₙ), and 𝗕 = (𝗯₁ 𝗯₂ … 𝗯ₚ) where 𝗮ᵢ and 𝗯ᵢ are iᵗʰ
columns of 𝗔 and 𝗕 respectively. Then

𝗔𝗕 = (𝗔𝗯₁ 𝗔𝗯₂ … 𝗔𝗯ₚ)

where 𝗔𝗯ᵢ is the iᵗʰ column of 𝗔𝗕.
By 4.1.16,

𝗔𝗯ᵢ ∈ column space of 𝗔 = span{𝗮₁, 𝗮₂, …, 𝗮ₙ}

By 3.2.10,

column space of 𝗔𝗕 = span{𝗔𝗯₁, 𝗔𝗯₂, …, 𝗔𝗯ₚ}
                   ⊆ span{𝗮₁, 𝗮₂, …, 𝗮ₙ} = column space of 𝗔

So

rank(𝗔𝗕) = dim(column space of 𝗔𝗕)
         <= dim(column space of 𝗔) = rank(𝗔)

Applying this same equation to 𝗕ᵀ𝗔ᵀ, we have rank(𝗕ᵀ𝗔ᵀ) <= rank(𝗕ᵀ)

Hence

rank(𝗔𝗕) = rank((𝗔𝗕)ᵀ)
         = rank(𝗕ᵀ𝗔ᵀ)
         <= rank(𝗕ᵀ)
         = rank(𝗕)

Thus we have shown that

[ rank(𝗔𝗕) <= rank(𝗔) ] ∧ [ rank(𝗔𝗕) <= rank(𝗕) ]
=> rank(𝗔𝗕) <= min{rank(𝗔), rank(𝗕)}
}}}

}}}

# Nullspaces and nullities                                       @ 147  `LA-4-3`
Definition 4.3.1 (nullspace) {{{

Let 𝗔 be an m × n matrix. The solution space of the homogeneous system of linear
equations 𝗔𝘅 = 0 is known as the nullspace of 𝗔.

The dimension of the nullspace of a matrix 𝗔 is called the nullity of 𝗔

Theorem 4.3.4 (dimension theorem)

Let 𝗔 be a matrix with n columns. Then

rank(𝗔) + nullity(𝗔) = n.
{{{
  Let R be a row-echelon form of 𝗔. The columns of R can be classified into two
  types: pivot columns and non-pivot columns. Since the rank of 𝗔 is the number
  of pivot columns in R and the nullity of 𝗔 is the number of non-pivot columns
  in R, the theorem follows.
}}}

}}}
Theorem 4.3.6 {{{

Suppose the system of linear equations 𝗔𝘅 = 𝗯 has a solution 𝘃. Then the
solution set of the system is given by

M = { 𝘂 + 𝘃 | 𝘂 ∈ nullspace of 𝗔 }

}}}
Remark 4.3.7 {{{

By 4.3.6, a consistent linear system 𝗔𝘅 = 𝗯 has only one solution if and only if
the nullspace of 𝗔 = {0}, where 0 is the zero vector.

}}}
}}}
# Orthogonality {{{                                              @ 156  `LA-5`

# The dot product                                                @ 156  `LA-5-1`
Definition 5.1.2 {{{

Let 𝘂 = (u₁, u₂, …, uₙ), 𝘃 = (v₁, v₂, …, vₙ) be two vectors in Rⁿ.

 1. The dot product (or inner product) of 𝘂 and 𝘃 is defined to be the value

    𝘂 · 𝘃 = u₁ v₁ + u₂ v₂ + … uₙ vₙ

 2. The norm (or length) of 𝘂 is defined to be

    ||𝘂|| = √(𝘂 · 𝘂) = √(u₁² + u₂² + … + uₙ²)

    In particular, vectors of norm 1 are called unit vectors.

 3. The distance between 𝘂 and 𝘃 is
    
    d(𝘂, 𝘃) = ||𝘂 - 𝘃|| = √[(u₁ - v₁)² + (u₂ - v₂)² + … + (uₙ - vₙ)²]

 4. The angle 𝜽 between 𝘂 and 𝘃 is

    cos^{-1}( 𝘂 · 𝘃 / ||𝘂|| ||𝘃|| )

    (note that -1 <= 𝘂 · 𝘃 / ||𝘂|| ||𝘃|| <= 1 so this angle is well-defined.)

}}}
Theorem 5.1.5 {{{

Let 𝘂, 𝘃, 𝘄 be vectors in Rⁿ and c a scalar. Then

 1. 𝘂 · 𝘃 = 𝘃 · u
 2. (𝘂 + 𝘃) · 𝘄 = 𝘂 · 𝘄 + 𝘃 · 𝘄,
    𝘄 · (𝘂 + 𝘃) = 𝘄 · 𝘂 + 𝘄 · 𝘃
 3. (cu) · 𝘃 = 𝘂 · (c𝘃) = c(𝘂 · 𝘃)
 4. ||cu|| = |c| ||𝘂||
 5. 𝘂 · 𝘂 >= 0 and 𝘂 · 𝘂 = 0 <=> 𝘂 = 0

}}}

# Orthogonal and orthonormal bases                               @ 159  `LA-5-2`
Definition 5.2.1 (orthogonal) {{{

 1. two vectors 𝘂 and 𝘃 in Rⁿ are called orthogonal if 𝘂 · 𝘃 = 0
 2. a set S of vectors in Rⁿ is called orthogonal if every pair of distinct
    vectors in S are orthogonal.
 3. a set S of vectors in Rⁿ is called orthonormal if S is orthogonal and every
    vector in S is a unit vector.

}}}
Remark 5.2.2 {{{

Given two non-zero vectors 𝘂 and v in Rⁿ, if they are orthogonal, then the
angle between them is equal to

arccos((𝘂 · 𝘃) / (||𝘂|| ||𝘃||)) = arccos(0) = π/2

Thus the concept of "orthogonal" in Rⁿ is the same as the concept of
"perpendicular" in R² and R³.

}}}
Theorem 5.2.4 {{{

Let S be an orthogonal set of non-zero vectors in a vector space. Then S is
linearly independent.

}}}
Definition 5.2.5 {{{

 1. A basis S for a vector space is called an orthogonal basis if S is
    orthogonal.
 2. A basis S for a vector space is called an orthonormal basis if S is
    orthonormal.

}}}
Remark 5.2.6 {{{

By Theorem 5.2.4 and Theorem 3.6.7, to determine whether a set S of non-zero
vectors in a vector space of dimension k is an orthogonal (respectively,
orthonormal) basis, we only need to check

 1. that S is orthogonal (respectively, orthonormal), and 
 2. |S| = k

}}}
Theorem 5.2.8 {{{

 1. If S = {𝘂₁, 𝘂₂, …, 𝘂ₖ} is an orthonormal basis for a vector space V,
    then for any vector 𝘄 in V,

    𝘄 = (𝘄·𝘂₁)/(𝘂₁·𝘂₁) 𝘂₁
       + (𝘄·𝘂₂)/(𝘂₂·𝘂₂) 𝘂₂
                ⋮
       + (𝘄·𝘂ₖ)/(𝘂ₖ·𝘂ₖ) 𝘂ₖ

    i.e. (𝘄)_S = (
           (𝘄·𝘂₁)/(𝘂₁·𝘂₁),
           (𝘄·𝘂₂)/(𝘂₂·𝘂₂),
                  ⋮         ,
           (𝘄·𝘂ₖ)/(𝘂ₖ·𝘂ₖ)
         )

 2. If T = {𝘃₁, 𝘃₂, …, 𝘃ₖ} is an orthonormal basis for a vector space V,
    then for any vector 𝘄 in V,

    𝘄 = (𝘄·𝘃₁)𝘃₁ + (𝘄·𝘃₂)𝘃₂ + … + (𝘄·𝘃ₖ)𝘃ₖ

    i.e. (𝘄)_T = (
           𝘄·𝘃₁,
           𝘄·𝘃₂,
             ⋮  ,
           𝘄·𝘃ₖ
         )

     }}}
Definition 5.2.10 {{{

Let V be a subspace of Rⁿ. A vector 𝘂 ∈ Rⁿ is said to be orthogonal (or
perpendicular) to V if 𝘂 is orthogonal to all vectors in V.

}}}
Remark 5.2.12 {{{

In general, if V = span{𝘂₁, 𝘂₂, …, 𝘂ₖ} is a subspace of Rⁿ, then a vector
𝘃 ∈ Rⁿ is orthogonal to V if and only if 𝘃·𝘂ᵢ = 0 for all i ∈ {1, 2, …, k}

}}}
Definition 5.2.13 {{{

Let V be a subspace of Rⁿ. Every vector 𝘂 ∈ Rⁿ can be written uniquely as

𝘂 = n + p

such that n is the vector orthogonal to V and p is a vector in V. The vector p
is called the (orthogonal) projection of 𝘂 onto V.

}}}
Theorem 5.2.15 {{{

Let V be a subspace of Rⁿ and 𝘄 a vector in Rⁿ.

 1. If {𝘂₁, 𝘂₂, …, 𝘂ₖ} is an orthonormal basis for a vector space V, then

    𝘄ₚ = (𝘄·𝘂₁)/(𝘂₁·𝘂₁) 𝘂₁
       + (𝘄·𝘂₂)/(𝘂₂·𝘂₂) 𝘂₂
                ⋮
       + (𝘄·𝘂ₖ)/(𝘂ₖ·𝘂ₖ) 𝘂ₖ

    is the projection of 𝘄 onto V.

 2. If {𝘃₁, 𝘃₂, …, 𝘃ₖ} is an orthonormal basis for a vector space V, then

    𝘄ₚ = (𝘄·𝘃₁)𝘃₁ + (𝘄·𝘃₂)𝘃₂ + … + (𝘄·𝘃ₖ)𝘃ₖ

    is the projection of 𝘄 onto V.

}}}
Remark 5.2.17 {{{

5.2.8 can be regarded as a particular case of 5.2.15 when 𝘄 is contained in V,
i.e. when 𝘄 = p and n = 0 in Definition 5.2.13.

}}}
Theorem 5.2.19 (Gram-Schmidt process) {{{

Let {𝘂₁, 𝘂₂, …, 𝘂ₖ} be a basis for a vector space V. Let

𝘃₁ = 𝘂₁

𝘃₂ = 𝘂₂ - (𝘂₂·𝘃₁)/(𝘃₁·𝘃₁) 𝘃₁

𝘃₃ = 𝘂₃ - (𝘂₃·𝘃₁)/(𝘃₁·𝘃₁) 𝘃₁ - (𝘂₃·𝘃₂)/(𝘃₂·𝘃₂) 𝘃₂

…

𝘃ₖ = 𝘂ₖ
     - (𝘂ₖ·𝘃₁)/(𝘃₁·𝘃₁) 𝘃₁
     - (𝘂ₖ·𝘃₂)/(𝘃₂·𝘃₂) 𝘃₂
                ⋮
     - (𝘂ₖ·𝘃ₖ-1)/(𝘃ₖ-1·𝘃ₖ-1) 𝘃ₖ-1

Then {𝘃₁, 𝘃₂, …, 𝘃ₖ} is an orthogonal basis for V. Furthermore, let

𝘄₁ = 1/||𝘃₁|| 𝘃₁
𝘄₂ = 1/||𝘃₂|| 𝘃₂
    ⋮
𝘄ₖ = 1/||𝘃ₖ|| 𝘃ₖ

Then {𝘄₁, 𝘄₂, …, 𝘄ₖ} is an orthonormal basis for V.

}}}

# Best approximations                                            @ 166  `LA-5-3`
Theorem 5.3.2 {{{

Let V be a subspace in Rⁿ. If 𝘂 is a vector in Rⁿ and p is the projection of u
onto V, then

d(𝘂, 𝗽) <= d(𝘂, 𝘃) for all 𝘃 ∈ V.

i.e. 𝗽 is the best approximation of 𝘂 in V.

}}}
Definition 5.3.6 {{{

Let 𝗔𝘅 = 𝗯 be a linear system where 𝗔 is an m × n matrix. A vector 𝘂 ∈ Rⁿ is
called a least squares solution to the linear system if ||𝗯 - 𝗔𝘂|| <= ||𝗯 - 𝗔𝘃||
for all 𝘃 ∈ Rⁿ.

}}}
Theorem 5.3.8 {{{

Let 𝗔𝘅 = 𝗯 be a linear system, where 𝗔 is an m × n matrix, and let 𝗽 be the
projection of 𝗯 onto the column space of 𝗔. Then

||𝗯 - 𝗽|| <= ||𝗯 - 𝗔𝘃|| for all 𝘃 ∈ Rⁿ.

i.e. 𝘂 is a least squares solution to 𝗔𝘅 = 𝗯 if and only if 𝗔𝘂 = 𝗽.

}}}
Theorem 5.3.10 (finding least squares) {{{

Let 𝗔𝘅 = 𝗯 be a linear system. Then 𝘂 is a least squares solution to 𝗔𝘅 = 𝗯 if
and only if 𝘂 is a solution to 𝗔ᵀ𝗔𝘅 = 𝗔ᵀ𝗯
{{{

Let 𝗔 = (𝗮₁ 𝗮₂ … 𝗮ₙ), where aᵢ is the iᵗʰ column of 𝗔, and let V be the
column space of 𝗔.
i.e. V = span{𝗮₁ 𝗮₂ … 𝗮ₙ} = { 𝗔𝘃 | 𝘃 ∈ Rⁿ }. Then

𝘂 is a least squares solution to 𝗔𝘅 = 𝗯
<=> 𝗔𝘂 is the projection of 𝗯 onto V
<=> 𝗯 - 𝗔𝘂 is orthogonal to V
<=> 𝗯 - 𝗔𝘂 is orthogonal to 𝗮₁, 𝗮₂, …, 𝗮ₙ
<=> 𝗮₁·(𝗯 - 𝗔𝘂) = 0, 𝗮₂·(𝗯 - 𝗔𝘂) = 0, …, 𝗮ₙ·(𝗯 - 𝗔𝘂) = 0.
<=> 𝗔ᵀ(𝗯 - 𝗔𝘂) = 0
<=> 𝗔ᵀ𝗔𝘂 = 𝗔ᵀ𝗯

}}}

}}}

# Orthogonal matrices                                            @ 171  `LA-5-4`
Definition 5.4.3 (orthogonal matrix) {{{

A square matrix 𝗔 is called orthogonal if 𝗔⁻¹ = 𝗔ᵀ
A non-square matrix cannot be an orthogonal matrix.

}}}
Remark 5.4.4 {{{

By Theorem 2.4.12, a square matrix 𝗔 is orthogonal if and only if
𝗔𝗔ᵀ = 𝗜 (or 𝗔ᵀ𝗔 = 𝗜)

}}}
Theorem 5.4.6 {{{

Let 𝗔 be a square matrix of order n. The following statements are equivalent:

 1. 𝗔 is an orthogonal matrix
 2. The rows of 𝗔 form an orthonormal basis for Rⁿ
 3. The columns of 𝗔 form an orthonormal basis for Rⁿ
{{{

1 <=> 2:

Let 𝗔 = (𝗮₁; 𝗮₂; … ; 𝗮ₙ) where aᵢ is the iᵗʰ row of 𝗔.

By Remark 5.2.6, it suffices to show that
(𝗔 is orthogonal) <=> 𝗮₁, 𝗮₂, …, 𝗮ₙ are orthonormal.

Observe that 𝗔𝗔ᵀ = [𝗮₁·𝗮₁ 𝗮₁·𝗮₂ … 𝗮₁·𝗮ₙ;
                    𝗮₂·𝗮₁ 𝗮₂·𝗮₂ … 𝗮₂·𝗮ₙ;
                                    ⋮
                    𝗮ₙ·𝗮₁ 𝗮ₙ·𝗮ₙ … 𝗮ₙ·𝗮ₙ]

By Remark 5.4.4,
𝗔 is orthogonal <=> 𝗔𝗔ᵀ = 𝗜
                <=> for all i, j, 𝗮ᵢ·𝗮ⱼ = 1, if i = j
                                          = 0, if i ≠ j
                <=> 𝗮₁, 𝗮₂, …, 𝗮ₙ are orthonormal.

The proof of 1 <=> 3 is similar except we compute 𝗔ᵀ𝗔 instead.

}}}

}}}
Theorem 5.4.7 {{{

Let S and T be two orthonormal bases for a vector space and let 𝗣 be the
transition matrix from S to T. Then 𝗣 is orthonormal and 𝗣ᵀ is the transition
matrix from T to S.

}}}
}}}
# Diagonalization {{{                                            @ 184  `LA-6`

# Eigenvalues and eigenvectors                                   @ 184  `LA-6-1`
Definition 6.1.3 (eigenvector, eigenvalue) {{{

Let 𝗔 be a square matrix of order n. A non-zero column vector 𝘂 in Rⁿ is called
an eigenvector of 𝗔 if

𝗔𝘂 = 𝝺𝘂

for some scalar 𝝺. The scalar 𝝺 is called an eigenvalue of 𝗔 and 𝘂 is said to be
an eigenvector of 𝗔 associated with the eigenvalue 𝝺.

}}}
Remark 6.1.5 {{{

Let 𝗔 be a square matrix of order n. Then

𝝺 is an eigenvalue of 𝗔.
<=> 𝗔𝘂 = 𝝺𝘂 for some non-zero column vector 𝘂 ∈ Rⁿ
<=> 𝝺𝘂 - 𝗔𝘂 = 0
<=> (𝝺𝗜 - 𝗔)𝘂 = 0
<=> the linear system (𝝺𝗜 - 𝗔)𝘅 = 0 has non-trivial solutions
<=> det(𝝺𝗜 - 𝗔) = 0

If expanded, det(𝝺𝗜 - 𝗔) is a polynomial in 𝝺 of degree n.

}}}
Definition 6.1.6 {{{

Let 𝗔 be a square matrix of order n. THe equation

det(𝝺𝗜 - 𝗔) = 0

is called the characteristic equation of 𝗔 and the polynomial

det(𝝺𝗜 - 𝗔)

is called the characteristic polynomial of 𝗔.

}}}
Theorem 6.1.8 (main theorem on invertible matrices, swiss army knife) {{{

Let 𝗔 be an n × n matrix. The following statements are equivalent.

 1. 𝗔 is invertible.
 2. The linear system 𝗔𝘅 = 0 has only the trivial solution.
 3. The reduced row-echelon form of 𝗔 is an identity matrix.
 4. 𝗔 can be expressed as a product of elementary matrices.
 5. det(𝗔) ≠ 0.
 6. The rows of 𝗔 form a basis for Rⁿ.
 7. The columns of 𝗔 form a basis for Rⁿ.
 8. rank(𝗔) = n.
 9. 0 is not an eigenvalue of 𝗔.
 10. Ax = b has a unique solution x, if consistent. `added`

}}}
Theorem 6.1.9 {{{

If 𝗔 is a triangular matrix (either upper or lower), the eigenvalues of 𝗔 are
the diagonal entries of 𝗔.
{{{

Suppose 𝗔 = (aᵢⱼ)ₙ×n is a triangular matrix. Then 𝝺𝗜 - 𝗔 is a triangular
matrix with diagonal entries 𝝺 - 𝗮₁₁, 𝝺 - 𝗮₂₂, …, 𝝺 - 𝗮ₙₙ. By 2.5.8,

det(𝝺𝗜 - 𝗔) = (𝝺 - 𝗮₁₁)(𝝺 - 𝗮₂₂) … (𝝺 - 𝗮ₙₙ)

Hence the diagonal entries 𝗮₁₁, 𝗮₂₂, …, 𝗮ₙₙ of 𝗔 are the eigenvalues of 𝗔.

}}}

}}}
Definition 6.1.11 {{{

Let 𝗔 be a square matrix of order n and 𝝺 an eigenvalue of 𝗔.

Then the solution space of the linear system (𝝺𝗜 - 𝗔)𝘅 = 0 is called the
eigenspace of 𝗔 associated with the eigenvalue 𝝺 and is denoted by E_𝝺.

Note that if 𝘂 is a non-zero vector in E_𝝺, then 𝘂 is an eigenvector of 𝗔
associated with the eigenvalue 𝝺.

}}}

# Diagonalization                                                @ 191  `LA-6-2`
Definition 6.2.1 (diagonalizability) {{{

A square matrix 𝗔 is called diagonalizable if there exists an invertible matrix
𝗣 such that 𝗣⁻¹𝗔𝗣 is a diagonal matrix. Here the matrix 𝗣 is said to
diagonalize 𝗔.

}}}
Theorem 6.2.3 (conditions for diagonalization) {{{

Let 𝗔 be a square matrix of order n. Then 𝗔 is diagonalizable if and only if 𝗔
has n linearly independent eigenvectors.

}}}
Algorithm 6.2.4 {{{

Given a square matrix 𝗔 of order n, we want to determine whether 𝗔 is
diagonalizable. Also, if 𝗔 is diagonalizable, find an invertible matrix 𝗣 such
that 𝗣⁻¹𝗔𝗣 is a diagonal matrix.

 1. Find all distinct eigenvalues 𝝺₁, 𝝺₂, …, 𝝺ₖ. (By 6.1.5, eigenvalues
    can be obtained by solving the characteristic equation det(𝝺𝗜 - 𝗔) = 0)
 2. For each eigenvalue 𝝺, find a basis S_𝝺, for the eigenspace E_𝝺.
 3. Let S = S_(𝝺₁) ∪ S_(𝝺₂) ∪ … ∪ S_(𝝺ₖ)
    a. if |S| < n, then 𝗔 is not diagonalizable.
    b. if |S| = n, say S = {𝘂₁, 𝘂₂, …, 𝘂ₙ}, then 𝗣 = (𝘂₁ 𝘂₂ … 𝘂ₙ) is
    an invertible matrix that diagonalizes 𝗔.

}}}
Theorem 6.2.7 {{{

Let 𝗔 be a square matrix of order n. If 𝗔 has n distinct eigenvalues, then 𝗔 is
diagonalizable.
{{{

Suppose 𝗔 has n distinct eigenvalues. In Step 2 of Algorithm 6.2.4, we can find
one eigenvector for each eigenvalue and hence we have n eigenvectors. By
6.2.5.3, these eigenvectors are linearly independent. So 𝗔 is diagonalizable.

}}}

}}}

# Orthogonal diagonalization                                     @ 198  `LA-6-3`
Definition 6.3.2 (orthogonal diagonalizability) {{{

if there exists an orthogonal matrix 𝗣 such that 𝗣ᵀ𝗔𝗣 is a diagonal matrix.
Here, 𝗣 is said to orthogonally diagonalize 𝗔.

}}}
Theorem 6.3.4 (condition for orthogonal diagonalizability) {{{

A square matrix is orthogonally diagonalizable if and only if it is symmetric.

}}}
Algorithm 6.3.5 {{{

Given a symmetric matrix 𝗔 of order n, we want to find an orthogonal matrix 𝗣
such that 𝗣ᵀ𝗔𝗣 is a diagonal matrix.

 1. Find all distinct eigenvalues 𝝺₁, 𝝺₂, …, 𝝺ₖ.
 2. For each eigenvalue 𝝺ᵢ,
    a. find a basis S_(𝝺ᵢ) for the eigenspace E_(𝝺ᵢ)
    b. use the Gram-Schmidt process (5.2.19) to transform S_(𝝺ᵢ) to an
    orthonormal basis T_(𝝺ᵢ)
 3. Let T = T_(𝝺₁) ∪ T_(𝝺₂) ∪ … ∪ T_(𝝺ₖ), say T = {𝘃₁, 𝘃₂, …, 𝘃ₙ}
    Then 𝗣 = (𝘃₁ 𝘃₂ … 𝘃ₙ) is an orthogonal matrix that diagonalizes 𝗔.

}}}
Remark 6.3.6 (about Algorithm 6.3.5){{{

 1. In Step 1, the eigenvalues of a symmetric matrix are always real numbers.
 2. Since 𝗔 is diagonalizable, by 6.2.5.2, we have the following result:
    Suppose the characteristic polynomial of the matrix 𝗔 can be factorized as

    det(𝝺𝗜 - 𝗔) = (𝝺 - 𝝺₁)^{r₁} · (𝝺 - 𝝺₂)^{r₂} · … · (𝝺 - 𝝺ₖ)^{rₖ}

    where 𝝺₁, 𝝺₂, …, 𝝺ₖ are distinct eigenvalues of 𝗔. Then in Step 2, for
    each eigenvalue 𝝺₁, dim(E_(𝝺ᵢ)) = rᵢ, i.e. |S_(𝝺ᵢ)| = |T_(𝝺ᵢ)| = rᵢ.
 3. In Step 3, the set T is always orthogonal.
 4. Since T is always orthogonal, by 5.4.6, the square matrix 𝗣 in Step 3 is
    always orthogonal.

}}}

# Quadratic forms and conic section                              @ 202  `LA-6-4`
Definition 6.4.1 {{{

The expression

Q(x₁, x₂, …, xₙ) = 𝝨_{i = 1}ⁿ { 𝝨_{j = i}ⁿ { qᵢⱼ xᵢ xⱼ }}
                  = q₁₁ (x₁)² + q₁₂ x₁ x₂ + … + q₁ₙ x₁ xₙ
                              + q₂₂ (x₂)² + … + q₂n x₁ xₙ
                                          + … 
                                              + qₙₙ (xₙ)²

where qᵢⱼ are real numbers, is called a quadratic form in n variables
x₁, x₂, …, xₙ.

Define an n × n symmetric matrix 𝗔 = (aᵢⱼ) such that

aᵢⱼ = qᵢᵢ      if i = j,
    = 1/2 qᵢⱼ  if i < j,
    = 1/2 qⱼᵢ  if i > j

and let 𝘅 = (x₁, x₂, …, xₙ)ᵀ Then

Q(x₁, x₂, …, xₙ)
= (x₁ x₂ … xₙ) * [   q₁₁   1/2·q₁₂ … 1/2·q₁ₙ; * [x₁;
                   1/2·q₁₂   q₂₂   … 1/2·q₂ₙ;    x₂;
                      ⋮            ⋱    ⋮        ⋮ ;
                   1/2·q₁ₙ 1/2·q₂ₙ …   qₙₙ  ]    xₙ]
= 𝘅ᵀ𝗔𝘅

Thus the quadratic form can be regarded as a mapping Q : Rⁿ -> R defined by

Q(𝘅) = 𝘅ᵀ𝗔𝘅  for 𝘅 ∈ Rⁿ

}}}
Definition 6.4.6 {{{

A quadratic equation in two variables x and y is an equation of the form

ax² + bxy + cy² + dx + ey = f

where a, b, c, d, e, f are real numbers and a, b, c are not all zero. We can
rewrite the equation in the form

[x y] * [  a   1/2·b; * [x;  +  [d e] * [x; = f
         1/2·b   c  ]    y]              y]

Let 𝘅 = [x; y], 𝗔 = [a 1/2·b; 1/2·b c], and 𝗯 = [d e]. Then the equation becomes

𝘅ᵀ𝗔𝘅 + 𝗯ᵀ𝘅 = f

The term ax² + bxy + cy² (= 𝘅ᵀ𝗔𝘅) is called the quadratic form associated with
the quadratic equation.

}}}

}}}
# Linear transformations {{{                                     @ 216  `LA-7`
# Linear transformations from Rⁿ to Rᵐ                           @ 216  `LA-7-1`

Definition 7.1.1

A linear transformation is a mapping T : Rⁿ -> Rᵐ.

T(x) = y,
for some x ∈ Rⁿ and y ∈ Rᵐ

The m × n matrix 𝗔 such that 𝗔𝘅 = 𝘆
is called the standard matrix for T.

Theorem 7.1.4

Let T : Rⁿ -> Rᵐ be a linear transformation.

 1. T(𝟬) = 𝟬
 2. If u₁, u₂, …, uₖ ∈ Rⁿ and c₁, c₂, …, cₖ ∈ R, then
    T(c₁u₁ + c₂u₂ + … + cₖuₖ) = c₁T(u₁) + c₂T(u₂) + … + cₖT(uₖ)

Definition 7.1.10

Let S : Rⁿ -> Rᵐ and T : Rᵐ -> Rᵏ be linear transformations.
The composition of T with S, T ∘ S, is a mapping from Rⁿ to Rᵏ

(T ∘ S)(u) = T(S(u)) for u ∈ Rⁿ

Theorem 7.1.11

If S : Rⁿ -> Rᵐ and T : Rᵐ -> Rᵏ are linear transformations, then
T ∘ S : Rⁿ -> Rᵏ is again a linear transformation.

Furthermore, if A and B are the standard matrices for the linear
transformations S and T respectively, then the standard matrix for
T ∘ S is given by BA.

# Ranges and kernels                                             @ 221  `LA-7-2`

Definition 7.2.1

Let T : Rⁿ -> Rᵐ be a linear transformation.
The range of T, denoted by R(T), is the set of images of T:

R(T) = { T(u) | u ∈ Rⁿ } ⊆ Rᵐ

Theorem 7.2.4

Let T : Rⁿ -> Rᵐ be a linear transformation and
A be the standard matrix for T. Then

R(T) = column space of A

which is a subspace of Rᵐ.

Definition 7.2.5

Let T be a linear transformation.
The dimension of R(T) is called the rank of T,
denoted by rank(T)

By Theorem 7.2.4, if A is the standard matrix of T,
then rank(T) = rank(A).

Theorem 7.2.9

Let T : Rⁿ -> Rᵐ be a linear transformation and
A be the standard matrix for T. Then

Ker(T) = nullspace of A

which is a subspace of Rᵐ.

Definition 7.2.10

Let T be a linear transformation.
The dimension of Ker(T) is called the nullity of T,
denoted by nullity(T).

By Theorem 7.2.9, if A is the standard matrix of T,
then nullity(T) = nullity(A).

Theorem 7.2.12 (Dimension Theorem for Linear Transformations)

If T : Rⁿ -> Rᵐ is a linear transformation, then

rank(T) + nullity(T) = n

# Geometric linear transformations                               @ 225  `LA-7-3`

}}}
